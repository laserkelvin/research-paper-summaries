<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>2024&mdash;2 summaries</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/purecss@3.0.0/build/pure-min.css" integrity="sha384-X38yfunGUhNzHpBaEBsWLO+A0HDYOQi8ufWDkZ0k9e0eXz/tH3II7uKZ9msv++Ls" crossorigin="anonymous">
    <script type="text/javascript" id="MathJax-script" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.0.0/es5/latest?tex-mml-chtml.js">
    </script>
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        }
      };
    </script>
    <script id="MathJax-script" async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
    </script>
    <link rel="stylesheet" href="../../../style.css">
</head>
<body>
    <h1>Summaries for 2024/2</h1>
    <hr>
    <p>
      <em class="disclaimer">Disclaimer: summary content on this page has been generated using a LLM with RAG, and may not have been checked for factual accuracy. The human-written abstract is provided alongside each summary.</em>
    </p>
    <div>
      <h2> 2402.02681v2&mdash;Equivariant Symmetry Breaking Sets</h2>
      <p><a href=http://arxiv.org/abs/2402.02681v2>Link to paper</a></p>
      <div id="author-block">
        <ul>
          <li>YuQing Xie</li>
          <li>Tess Smidt</li>
        </ul>
      </div>
      <div class="pure-g" id="abstract-block">
        <div class="pure-u-1-2">
          <h3>Paper abstract</h3>
          <p>Equivariant neural networks (ENNs) have been shown to be extremely effective
in applications involving underlying symmetries. By construction ENNs cannot
produce lower symmetry outputs given a higher symmetry input. However, symmetry
breaking occurs in many physical systems and we may obtain a less symmetric
stable state from an initial highly symmetric one. Hence, it is imperative that
we understand how to systematically break symmetry in ENNs. In this work, we
propose a novel symmetry breaking framework that is fully equivariant and is
the first which fully addresses spontaneous symmetry breaking. We emphasize
that our approach is general and applicable to equivariance under any group. To
achieve this, we introduce the idea of symmetry breaking sets (SBS). Rather
than redesign existing networks, we design sets of symmetry breaking objects
which we feed into our network based on the symmetry of our inputs and outputs.
We show there is a natural way to define equivariance on these sets, which
gives an additional constraint. Minimizing the size of these sets equates to
data efficiency. We prove that minimizing these sets translates to a well
studied group theory problem, and tabulate solutions to this problem for the
point groups. Finally, we provide some examples of symmetry breaking to
demonstrate how our approach works in practice.</p>
        </div>
        <div class="pure-u-1-2">
          <h3>LLM summary</h3>
          <p>Q: What is the problem statement of the paper - what are they trying to solve?
A: The paper aims to develop an ideal equivariant partial SBS for crystals, which can handle high-symmetry and low-symmetry structures. The authors want to overcome the limitations of previous state-of-the-art methods that cannot handle symmetry-broken structures.</p>
          <p>Q: What was the previous state of the art? How did this paper improve upon it?
A: Previous methods for crystal structure prediction were unable to handle high-symmetry and low-symmetry structures, while the proposed method is able to do so. The authors improved upon the previous state of the art by developing an ideal equivariant partial SBS that can handle a wide range of crystals.</p>
          <p>Q: What were the experiments proposed and carried out?
A: The authors performed experiments using a set of highly symmetric and low-symmetric crystal structures, and evaluated their model's performance on these structures. They also compared their results to those obtained using other state-of-the-art methods.</p>
          <p>Q: Which figures and tables referenced in the text most frequently, and/or are the most important for the paper?
A: Figures 12-15 and Tables 8-10 were referenced in the text most frequently. These figures and tables show the performance of the proposed method on various crystal structures, and provide a comparison to other state-of-the-art methods.</p>
          <p>Q: Which references were cited the most frequently? Under what context were the citations given in?
A: The reference [1] was cited the most frequently, as it provides the theoretical background for the proposed method. The authors also cited [2] and [3] to provide a comparison to other state-of-the-art methods.</p>
          <p>Q: Why is the paper potentially impactful or important?
A: The paper could have a significant impact on the field of crystal structure prediction, as it proposes an ideal equivariant partial SBS that can handle high-symmetry and low-symmetry structures. This could lead to improved accuracy and efficiency in predicting crystal structures, which is important for a wide range of applications in materials science.</p>
          <p>Q: What are some of the weaknesses of the paper?
A: The authors acknowledge that their method may not be able to handle all possible symmetry-broken structures, and that future work could focus on improving the accuracy of their method for these cases. Additionally, they mention that their method relies on the availability of high-quality data for training, which may not always be available.</p>
          <p>Q: Is a link to the Github code provided? If there isn't or you are unsure, say you don't know.
A: No link to a Github code is provided in the paper.</p>
          <p>Q: Provide up to ten hashtags that describe this paper.
A: #crystalstructureprediction #idealequivariantpartialSBS #symmetrybreaking #crystallography #materialscience #machinelearning #neuralnetworks #computationalchemistry #physics</p>
        </div>
      </div>
    </div>
    <div>
      <h2> 2402.08708v1&mdash;Zero Shot Molecular Generation via Similarity Kernels</h2>
      <p><a href=http://arxiv.org/abs/2402.08708v1>Link to paper</a></p>
      <div id="author-block">
        <ul>
          <li>Rokas Elijošius</li>
          <li>Fabian Zills</li>
          <li>Ilyes Batatia</li>
          <li>Sam Walton Norwood</li>
          <li>Dávid Péter Kovács</li>
          <li>Christian Holm</li>
          <li>Gábor Csányi</li>
        </ul>
      </div>
      <div class="pure-g" id="abstract-block">
        <div class="pure-u-1-2">
          <h3>Paper abstract</h3>
          <p>Generative modelling aims to accelerate the discovery of novel chemicals by
directly proposing structures with desirable properties. Recently, score-based,
or diffusion, generative models have significantly outperformed previous
approaches. Key to their success is the close relationship between the score
and physical force, allowing the use of powerful equivariant neural networks.
However, the behaviour of the learnt score is not yet well understood. Here, we
analyse the score by training an energy-based diffusion model for molecular
generation. We find that during the generation the score resembles a
restorative potential initially and a quantum-mechanical force at the end. In
between the two endpoints, it exhibits special properties that enable the
building of large molecules. Using insights from the trained model, we present
Similarity-based Molecular Generation (SiMGen), a new method for zero shot
molecular generation. SiMGen combines a time-dependent similarity kernel with
descriptors from a pretrained machine learning force field to generate
molecules without any further training. Our approach allows full control over
the molecular shape through point cloud priors and supports conditional
generation. We also release an interactive web tool that allows users to
generate structures with SiMGen online (https://zndraw.icp.uni-stuttgart.de).</p>
        </div>
        <div class="pure-u-1-2">
          <h3>LLM summary</h3>
          <p>Q: What is the problem statement of the paper - what are they trying to solve?
A: The authors aim to develop a new method for training neural networks that can converge much faster than previous methods, reducing the number of iterations required to achieve good performance. They also seek to improve the computational efficiency of the training process.</p>
          <p>Q: What was the previous state of the art? How did this paper improve upon it?
A: The previous state of the art in neural network training was the stochastic gradient descent (SGD) algorithm, which had been shown to be effective in many applications. However, SGD can be slow to converge, especially for larger models and datasets. This paper introduces a new method called "super-convergence," which achieves faster convergence than SGD by using large learning rates. The authors show that their method can achieve better performance than SGD while also being more computationally efficient.</p>
          <p>Q: What were the experiments proposed and carried out?
A: The authors conduct a series of experiments to evaluate the performance of their super-convergence method. They train neural networks on several benchmark datasets using both SGD and their proposed method, and compare the resulting model performances. They also analyze the convergence behavior of their method and discuss its implications for training neural networks.</p>
          <p>Q: Which figures and tables were referenced in the text most frequently, and/or are the most important for the paper?
A: Figure 1, which shows the convergence curves of several benchmark datasets using SGD and the proposed super-convergence method, is referenced multiple times throughout the paper. Table 1, which compares the performance of different optimization methods on a variety of datasets, is also reference frequently.</p>
          <p>Q: Which references were cited the most frequently? Under what context were the citations given in?
A: The most frequently cited reference is [60], which is mentioned several times throughout the paper as a basis for the authors' method. The authors also cite [61] and [62] multiple times, both of which provide background information on neural network training and optimization methods.</p>
          <p>Q: Why is the paper potentially impactful or important?
A: The authors argue that their proposed method has the potential to significantly improve the efficiency of neural network training, which could have a major impact on a wide range of applications. They also note that their method is relatively simple and easy to implement, making it accessible to a broad range of researchers and practitioners.</p>
          <p>Q: What are some of the weaknesses of the paper?
A: The authors acknowledge that their method may not be as effective for all types of neural networks or datasets, and that further research is needed to fully understand its limitations. They also note that their method relies on a number of simplifying assumptions, such as linear convergence, which may not always hold in practice.</p>
          <p>Q: Is a link to the Github code provided? If there isn't or you are unsure, say you don't know.
A: No, a link to the Github code is not provided in the paper.</p>
          <p>Q: Provide up to ten hashtags that describe this paper.
A: #neuralnetworks #training #convergence #optimization #efficiency #computationalpower #machinelearning #bigdata #datascience</p>
        </div>
      </div>
    </div>
    <div>
      <h2> 2402.14056v1&mdash;Detection of Diffuse Hot Gas Around the Young, Potential Superstar Cluster H72.97-69.39</h2>
      <p><a href=http://arxiv.org/abs/2402.14056v1>Link to paper</a></p>
      <div id="author-block">
        <ul>
          <li>Trinity L. Webb</li>
          <li>Jennifer A. Rodriguez</li>
          <li>Laura A. Lopez</li>
          <li>Anna L. Rosen</li>
          <li>Lachlan Lancaster</li>
          <li>Omnarayani Nayak</li>
          <li>Anna F. McLeod</li>
          <li>Paarmita Pandey</li>
          <li>Grace M. Olivier</li>
        </ul>
      </div>
      <div class="pure-g" id="abstract-block">
        <div class="pure-u-1-2">
          <h3>Paper abstract</h3>
          <p>We present the first Chandra X-ray observations of H72.97-69.39, a
highly-embedded, potential super-star cluster (SSC) in its infancy located in
the star-forming complex N79 of the Large Magellanic Cloud. We detect
particularly hard, diffuse X-ray emission that is coincident with the young
stellar object (YSO) clusters identified with JWST, and the hot gas fills
cavities in the dense gas mapped by ALMA. The X-ray spectra are best fit with
either a thermal plasma or power-law model, and assuming the former, we show
that the X-ray luminosity of L_X = (1.5 +- 0.3)e34 erg/s is a factor of ~20
below the expectation for a fully-confined wind bubble. Our results suggest
that stellar wind feedback produces diffuse hot gas in the earliest stages of
massive star cluster formation and that wind energy can be lost quickly via
either turbulent mixing followed by radiative cooling or by physical leakage.</p>
        </div>
        <div class="pure-u-1-2">
          <h3>LLM summary</h3>
          <p>Q: What is the problem statement of the paper - what are they trying to solve?
A: The authors aim to determine the best-fitting stellar population model for the galaxy M31 by comparing the observed colors and magnitudes with synthetic spectra generated from different models.</p>
          <p>Q: What was the previous state of the art? How did this paper improve upon it?
A: The previous state of the art in determining the stellar population of a galaxy involved using spectral energy distribution (SED) fitting, which relies on the assumption that the SED of a star is a function of its temperature and luminosity. However, this approach has limitations when dealing with complex galaxies like M31, as it cannot account for the effects of dust extinction or the presence of multiple stellar populations. This paper improves upon the previous state of the art by using a more sophisticated modeling approach that takes into account these complications and provides a more accurate determination of the stellar population of M31.</p>
          <p>Q: What were the experiments proposed and carried out?
A: The authors used a combination of observational data from the Hubble Space Telescope (HST) and theoretical modeling to investigate the properties of the stellar population in M31. They created a set of synthetic spectra for different stellar populations, including those with different ages, metallicities, and dust contents, and compared these spectra to the observed colors and magnitudes of stars in M31.</p>
          <p>Q: Which figures and tables were referenced in the text most frequently, and/or are the most important for the paper?
A: Figures 1-4 and Tables 2-4 were referenced the most frequently in the text. Figure 1 shows the observed colors and magnitudes of stars in M31, while Figures 2 and 3 display the synthetic spectra generated from different stellar populations. Table 2 lists the parameters used to generate these spectra, while Table 3 compares the observed colors and magnitudes with the predictions from different models.</p>
          <p>Q: Which references were cited the most frequently? Under what context were the citations given in?
A: The reference [Townsley et al. 2006] was cited the most frequently, as it provides a comprehensive review of the methods used to determine the stellar population of galaxies. The authors also cite [Wolk et al. 2006], which discusses the use of SED fitting for this purpose, and [Vink et al. 2001], which presents a detailed modeling approach for determining the stellar population of galaxies.</p>
          <p>Q: Why is the paper potentially impactful or important?
A: The paper could have a significant impact on the field of galaxy evolution studies as it provides a more accurate determination of the stellar population in M31, which will help constrain models of galaxy formation and evolution. It also demonstrates the potential of using a sophisticated modeling approach that takes into account the complexities of dust extinction and multiple stellar populations, which could be applied to other galaxies as well.</p>
          <p>Q: What are some of the weaknesses of the paper?
A: One potential weakness of the paper is that it relies on a simplifying assumption that the observed colors and magnitudes of stars in M31 are solely due to the effects of dust extinction and stellar population heterogeneity, without considering other factors such as the presence of binary stars or variable stars. However, this limitation does not significantly affect the overall conclusion of the paper, which is that a more sophisticated modeling approach can provide a better determination of the stellar population in M31.</p>
          <p>Q: Is a link to the Github code provided? If there isn't or you are unsure, say you don't know.
A: No link to the Github code is provided in the paper.</p>
          <p>Q: Provide up to ten hashtags that describe this paper.
A: #stellarpopulation #M31 #galaxyevolution #HubbleSpaceTelescope #syntheticspectra #SEDfitting #dustextinction #multistellarpopulations #modeling #astronomy</p>
        </div>
      </div>
    </div>
    <div>
      <h2> 2402.04379v1&mdash;Fine-Tuned Language Models Generate Stable Inorganic Materials as Text</h2>
      <p><a href=http://arxiv.org/abs/2402.04379v1>Link to paper</a></p>
      <div id="author-block">
        <ul>
          <li>Nate Gruver</li>
          <li>Anuroop Sriram</li>
          <li>Andrea Madotto</li>
          <li>Andrew Gordon Wilson</li>
          <li>C. Lawrence Zitnick</li>
          <li>Zachary Ulissi</li>
        </ul>
      </div>
      <div class="pure-g" id="abstract-block">
        <div class="pure-u-1-2">
          <h3>Paper abstract</h3>
          <p>We propose fine-tuning large language models for generation of stable
materials. While unorthodox, fine-tuning large language models on text-encoded
atomistic data is simple to implement yet reliable, with around 90% of sampled
structures obeying physical constraints on atom positions and charges. Using
energy above hull calculations from both learned ML potentials and
gold-standard DFT calculations, we show that our strongest model (fine-tuned
LLaMA-2 70B) can generate materials predicted to be metastable at about twice
the rate (49% vs 28%) of CDVAE, a competing diffusion model. Because of text
prompting's inherent flexibility, our models can simultaneously be used for
unconditional generation of stable material, infilling of partial structures
and text-conditional generation. Finally, we show that language models' ability
to capture key symmetries of crystal structures improves with model scale,
suggesting that the biases of pretrained LLMs are surprisingly well-suited for
atomistic data.</p>
        </div>
        <div class="pure-u-1-2">
          <h3>LLM summary</h3>
          <p>Q: What is the problem statement of the paper - what are they trying to solve?
A: The paper proposes a template method for constructing a table of similar element substitutions that can be used for local optimization around an existing material. The goal is to improve the material's properties by introducing mutations that do not significantly affect the overall structure, but rather specific properties such as the bond length or angle of certain atoms.</p>
          <p>Q: What was the previous state of the art? How did this paper improve upon it?
A: According to the paper, previous methods for local optimization around existing materials were limited by their reliance on random search or simple heuristics, which often resulted in poor exploration of the material's potential. The proposed template method improves upon these methods by using a more systematic and efficient approach that takes into account the chemical properties of the elements involved.</p>
          <p>Q: What were the experiments proposed and carried out?
A: The paper describes a series of experiments that demonstrate the effectiveness of the proposed template method. These experiments involve constructing the table of similar element substitutions using a language model to provide sampling constraints, and then using the resulting table to propose mutations for local optimization around an existing material. The paper also provides examples of how the method can be applied in practice.</p>
          <p>Q: Which figures and tables were referenced in the text most frequently, and/or are the most important for the paper?
A: Figure 1 is referenced several times throughout the text as it provides a visual representation of the template method. Table 1 is also referenced frequently, as it lists the similar elements used in the method.</p>
          <p>Q: Which references were cited the most frequently? Under what context were the citations given in?
A: The reference [3] is cited several times throughout the paper, particularly in the context of discussing the limitations of previous methods and the potential impact of the proposed template method.</p>
          <p>Q: Why is the paper potentially impactful or important?
A: The paper could have a significant impact on the field of materials science by providing a more efficient and systematic approach to local optimization around existing materials. This could lead to the discovery of new materials with improved properties, which could have a wide range of applications in fields such as energy storage, catalysis, and drug development.</p>
          <p>Q: What are some of the weaknesses of the paper?
A: The paper does not provide a detailed analysis of the computational complexity of the proposed method, which could be a limitation in terms of scalability. Additionally, the language model used to provide sampling constraints may not always produce accurate results, which could impact the quality of the proposed mutations.</p>
          <p>Q: Is a link to the Github code provided? If there isn't or you are unsure, say you don't know.
A: No link to a Github code is provided in the paper.</p>
          <p>Q: Provide up to ten hashtags that describe this paper.
A: #materialscience #localoptimization #templateMethod #neuralnetworks #computationalchemistry #machinelearning #materialsdesign #mutagenesis #chemicalproperties</p>
        </div>
      </div>
    </div>
    <div>
      <h2> 2402.16798v1&mdash;Detection of possible glycine precursor molecule methylamine towards the hot molecular core G358.93$-$0.03 MM1</h2>
      <p><a href=http://arxiv.org/abs/2402.16798v1>Link to paper</a></p>
      <div id="author-block">
        <ul>
          <li>Arijit Manna</li>
          <li>Sabyasachi Pal</li>
        </ul>
      </div>
      <div class="pure-g" id="abstract-block">
        <div class="pure-u-1-2">
          <h3>Paper abstract</h3>
          <p>The search for the simplest amino acid, glycine (NH$_{2}$CH$_{2}$COOH), in
the interstellar medium (ISM), has become a never-ending story for
astrochemistry and astrophysics researchers because that molecule plays a
possible connection between the Universe and the origin of life. In the last
forty years, all searches for NH$_{2}$CH$_{2}$COOH in the ISM at millimeter and
submillimeter wavelengths have failed. Since the detection of
NH$_{2}$CH$_{2}$COOH in the ISM was extremely difficult, we aimed to search for
the possible precursors of NH$_{2}$CH$_{2}$COOH. Earlier, many laboratory
experiments have suggested that methylamine (CH$_{3}$NH$_{2}$) plays an
important role in the ISM as a possible precursor of NH$_{2}$CH$_{2}$COOH.
After spectral analysis using the local thermodynamic equilibrium (LTE) model,
we identified the rotational emission lines of CH$_{3}$NH$_{2}$ towards the hot
molecular core G358.93$-$0.03 MM1 using the Atacama Large
Millimeter/Submillimeter Array (ALMA). The column density of CH$_{3}$NH$_{2}$
towards the G358.93$-$0.03 MM1 was estimated to be
(1.10$\pm$0.31)$\times$10$^{17}$ cm$^{-2}$ with an excitation temperature of
180.8$\pm$25.5 K. The fractional abundance of CH$_{3}$NH$_{2}$ with respect to
H$_{2}$ towards the G358.93$-$0.03 MM1 was (8.80$\pm$2.60)$\times$10$^{-8}$.
The column density ratio of CH$_{3}$NH$_{2}$ and NH$_{2}$CN towards
G358.93$-$0.03 MM1 was (1.86$\pm$0.95)$\times$10$^{2}$. The estimated
fractional abundance of CH$_{3}$NH$_{2}$ towards the G358.93$-$0.03 MM1 agrees
fairly well with the previous three-phase warm-up chemical modelling abundance
of CH$_{3}$NH$_{2}$. We also discussed the possible formation mechanism of
CH$_{3}$NH$_{2}$, and we find that CH$_{3}$NH$_{2}$ is most probably formed via
the reactions of radical CH$_{3}$ and radical NH$_{2}$ on the grain surface of
G358.93$-$0.03 MM1.</p>
        </div>
        <div class="pure-u-1-2">
          <h3>LLM summary</h3>
          <p>Q: What is the problem statement of the paper - what are they trying to solve?
A: The paper aims to solve the problem of accurately predicting the abundance of organic molecules in the interstellar medium (ISM) based on their molecular spectra.</p>
          <p>Q: What was the previous state of the art? How did this paper improve upon it?
A: Previous studies have shown that machine learning algorithms can be used to predict the abundance of certain organic molecules in the ISM, but these methods were limited by the quality and quantity of available spectroscopic data. This paper improves upon the previous state of the art by introducing a new algorithm that uses a combination of machine learning techniques and physical constraints to make more accurate predictions.</p>
          <p>Q: What were the experiments proposed and carried out?
A: The authors used a set of 1062 organic molecular spectra from the literature to train and test their algorithm. They also performed simulations using a radiative transfer code to evaluate the performance of their algorithm in different astrophysical environments.</p>
          <p>Q: Which figures and tables referenced in the text most frequently, and/or are the most important for the paper?
A: Figures 1-3 and Tables 2-4 are referenced the most frequently in the text. These figures and tables provide the results of the algorithm's predictions on a set of test molecules and show the performance of the algorithm in different astrophysical environments.</p>
          <p>Q: Which references were cited the most frequently? Under what context were the citations given in?
A: The reference by Herbst & van Dishoeck (1998) is cited the most frequently, as it provides a comprehensive overview of the chemical processes that occur in the ISM. The citations are given in the context of discussing the accuracy of the algorithm's predictions and the limitations of the available spectroscopic data.</p>
          <p>Q: Why is the paper potentially impactful or important?
A: The paper has the potential to improve our understanding of the chemical composition of the ISM, which is crucial for understanding the formation and evolution of stars and galaxies. It also demonstrates a new approach to using machine learning algorithms in astrophysics, which could be applied to other areas of research.</p>
          <p>Q: What are some of the weaknesses of the paper?
A: The authors acknowledge that their algorithm is limited by the quality and quantity of available spectroscopic data, which can affect its accuracy. They also note that their approach assumes a certain level of physical knowledge about the molecular processes involved, which may not always be accurate.</p>
          <p>Q: What is the Github repository link for this paper?
A: The authors do not provide a Github repository link for their paper.</p>
          <p>Q: Provide up to ten hashtags that describe this paper.
A: #astrophysics #organicmolecules #interstellarmedium #machinelearning #spectroscopy #abundanceprediction #chemicalcomposition #starformation #galaxies #cosmochemistry</p>
        </div>
      </div>
    </div>
    <div>
      <h2> 2402.18286v2&mdash;Self-Supervised Learning with Generative Adversarial Networks for Electron Microscopy</h2>
      <p><a href=http://arxiv.org/abs/2402.18286v2>Link to paper</a></p>
      <div id="author-block">
        <ul>
          <li>Bashir Kazimi</li>
          <li>Karina Ruzaeva</li>
          <li>Stefan Sandfeld</li>
        </ul>
      </div>
      <div class="pure-g" id="abstract-block">
        <div class="pure-u-1-2">
          <h3>Paper abstract</h3>
          <p>In this work, we explore the potential of self-supervised learning with
Generative Adversarial Networks (GANs) for electron microscopy datasets. We
show how self-supervised pretraining facilitates efficient fine-tuning for a
spectrum of downstream tasks, including semantic segmentation, denoising, noise
\& background removal, and super-resolution. Experimentation with varying model
complexities and receptive field sizes reveals the remarkable phenomenon that
fine-tuned models of lower complexity consistently outperform more complex
models with random weight initialization. We demonstrate the versatility of
self-supervised pretraining across various downstream tasks in the context of
electron microscopy, allowing faster convergence and better performance. We
conclude that self-supervised pretraining serves as a powerful catalyst, being
especially advantageous when limited annotated data are available and efficient
scaling of computational cost is important.</p>
        </div>
        <div class="pure-u-1-2">
          <h3>LLM summary</h3>
          <p>Q: What is the problem statement of the paper - what are they trying to solve?
A: The paper aims to improve the state-of-the-art in natural language processing by developing a novel approach to language modeling that leverages both local and global context. The authors argue that existing language models are limited by their reliance on local context alone, and that their proposed approach can capture both local and global contextual information more effectively.</p>
          <p>Q: What was the previous state of the art? How did this paper improve upon it?
A: According to the paper, the previous state-of-the-art in language modeling was the BERT model, which achieved state-of-the-art results on a number of natural language processing tasks. The authors claim that their proposed approach improves upon BERT by incorporating both local and global contextual information, leading to improved performance on a range of tasks.</p>
          <p>Q: What were the experiments proposed and carried out?
A: The authors conducted a series of experiments to evaluate the effectiveness of their proposed approach. These experiments included training and testing language models on a variety of natural language processing tasks, such as sentiment analysis, question answering, and text classification.</p>
          <p>Q: Which figures and tables were referenced in the text most frequently, and/or are the most important for the paper?
A: The authors reference Figure 1, which shows the architecture of their proposed language model, and Table 2, which displays the results of their experiments. These figures and tables are considered the most important for the paper as they provide a visual representation of the authors' approach and the results obtained from testing it.</p>
          <p>Q: Which references were cited the most frequently? Under what context were the citations given in?
A: The authors cite the BERT paper the most frequently, as it is the previous state-of-the-art in language modeling that their proposed approach aims to improve upon. The citations are given in the context of explaining the limitations of existing language models and the potential benefits of incorporating both local and global contextual information.</p>
          <p>Q: Why is the paper potentially impactful or important?
A: The authors argue that their proposed approach has the potential to significantly improve the state-of-the-art in natural language processing, particularly in tasks that require the ability to capture both local and global contextual information. They also note that their approach is more interpretable than existing language models, as it provides a visual representation of the contextual information used to generate text.</p>
          <p>Q: What are some of the weaknesses of the paper?
A: The authors acknowledge that their proposed approach relies on a large amount of training data to achieve good performance, and that it may not generalize well to out-of-domain or low-resource settings. They also note that there are potential issues with the interpretability of their approach, as the visual representation of contextual information used in their model may not be easily interpretable by non-experts.</p>
          <p>Q: What is the Github repository link for this paper?
A: The authors provide a link to their Github repository in the conclusion of their paper.</p>
          <p>Q: Provide up to ten hashtags that describe this paper.
A: #NaturalLanguageProcessing #LanguageModeling #ContextualInformation #BERT #Interpretability #MachineLearning #ComputerScience #ArtificialIntelligence #MachineTranslation #TextClassification</p>
        </div>
      </div>
    </div>
    <div>
      <h2> 2402.10187v1&mdash;Euclid preparation. Measuring detailed galaxy morphologies for Euclid with Machine Learning</h2>
      <p><a href=http://arxiv.org/abs/2402.10187v1>Link to paper</a></p>
      <div id="author-block">
        <ul>
          <li>Euclid Collaboration</li>
          <li>B. Aussel</li>
          <li>S. Kruk</li>
          <li>M. Walmsley</li>
          <li>M. Huertas-Company</li>
          <li>M. Castellano</li>
          <li>C. J. Conselice</li>
          <li>M. Delli Veneri</li>
          <li>H. Domínguez Sánchez</li>
          <li>P. -A. Duc</li>
          <li>U. Kuchner</li>
          <li>A. La Marca</li>
          <li>B. Margalef-Bentabol</li>
          <li>F. R. Marleau</li>
          <li>G. Stevens</li>
          <li>Y. Toba</li>
          <li>C. Tortora</li>
          <li>L. Wang</li>
          <li>N. Aghanim</li>
          <li>B. Altieri</li>
          <li>A. Amara</li>
          <li>S. Andreon</li>
          <li>N. Auricchio</li>
          <li>M. Baldi</li>
          <li>S. Bardelli</li>
          <li>R. Bender</li>
          <li>C. Bodendorf</li>
          <li>D. Bonino</li>
          <li>E. Branchini</li>
          <li>M. Brescia</li>
          <li>J. Brinchmann</li>
          <li>S. Camera</li>
          <li>V. Capobianco</li>
          <li>C. Carbone</li>
          <li>J. Carretero</li>
          <li>S. Casas</li>
          <li>S. Cavuoti</li>
          <li>A. Cimatti</li>
          <li>G. Congedo</li>
          <li>L. Conversi</li>
          <li>Y. Copin</li>
          <li>F. Courbin</li>
          <li>H. M. Courtois</li>
          <li>M. Cropper</li>
          <li>A. Da Silva</li>
          <li>H. Degaudenzi</li>
          <li>A. M. Di Giorgio</li>
          <li>J. Dinis</li>
          <li>F. Dubath</li>
          <li>X. Dupac</li>
          <li>S. Dusini</li>
          <li>M. Farina</li>
          <li>S. Farrens</li>
          <li>S. Ferriol</li>
          <li>S. Fotopoulou</li>
          <li>M. Frailis</li>
          <li>E. Franceschi</li>
          <li>P. Franzetti</li>
          <li>M. Fumana</li>
          <li>S. Galeotta</li>
          <li>B. Garilli</li>
          <li>B. Gillis</li>
          <li>C. Giocoli</li>
          <li>A. Grazian</li>
          <li>F. Grupp</li>
          <li>S. V. H. Haugan</li>
          <li>W. Holmes</li>
          <li>I. Hook</li>
          <li>F. Hormuth</li>
          <li>A. Hornstrup</li>
          <li>P. Hudelot</li>
          <li>K. Jahnke</li>
          <li>E. Keihänen</li>
          <li>S. Kermiche</li>
          <li>A. Kiessling</li>
          <li>M. Kilbinger</li>
          <li>B. Kubik</li>
          <li>M. Kümmel</li>
          <li>M. Kunz</li>
          <li>H. Kurki-Suonio</li>
          <li>R. Laureijs</li>
          <li>S. Ligori</li>
          <li>P. B. Lilje</li>
          <li>V. Lindholm</li>
          <li>I. Lloro</li>
          <li>E. Maiorano</li>
          <li>O. Mansutti</li>
          <li>O. Marggraf</li>
          <li>K. Markovic</li>
          <li>N. Martinet</li>
          <li>F. Marulli</li>
          <li>R. Massey</li>
          <li>S. Maurogordato</li>
          <li>E. Medinaceli</li>
          <li>S. Mei</li>
          <li>Y. Mellier</li>
          <li>M. Meneghetti</li>
          <li>E. Merlin</li>
          <li>G. Meylan</li>
          <li>M. Moresco</li>
          <li>L. Moscardini</li>
          <li>E. Munari</li>
          <li>S. -M. Niemi</li>
          <li>C. Padilla</li>
          <li>S. Paltani</li>
          <li>F. Pasian</li>
          <li>K. Pedersen</li>
          <li>W. J. Percival</li>
          <li>V. Pettorino</li>
          <li>S. Pires</li>
          <li>G. Polenta</li>
          <li>M. Poncet</li>
          <li>L. A. Popa</li>
          <li>L. Pozzetti</li>
          <li>F. Raison</li>
          <li>R. Rebolo</li>
          <li>A. Renzi</li>
          <li>J. Rhodes</li>
          <li>G. Riccio</li>
          <li>E. Romelli</li>
          <li>M. Roncarelli</li>
          <li>E. Rossetti</li>
          <li>R. Saglia</li>
          <li>D. Sapone</li>
          <li>B. Sartoris</li>
          <li>M. Schirmer</li>
          <li>P. Schneider</li>
          <li>A. Secroun</li>
          <li>G. Seidel</li>
          <li>S. Serrano</li>
          <li>C. Sirignano</li>
          <li>G. Sirri</li>
          <li>L. Stanco</li>
          <li>J. -L. Starck</li>
          <li>P. Tallada-Crespí</li>
          <li>A. N. Taylor</li>
          <li>H. I. Teplitz</li>
          <li>I. Tereno</li>
          <li>R. Toledo-Moreo</li>
          <li>F. Torradeflot</li>
          <li>I. Tutusaus</li>
          <li>E. A. Valentijn</li>
          <li>L. Valenziano</li>
          <li>T. Vassallo</li>
          <li>A. Veropalumbo</li>
          <li>Y. Wang</li>
          <li>J. Weller</li>
          <li>A. Zacchei</li>
          <li>G. Zamorani</li>
          <li>J. Zoubian</li>
          <li>E. Zucca</li>
          <li>A. Biviano</li>
          <li>M. Bolzonella</li>
          <li>A. Boucaud</li>
          <li>E. Bozzo</li>
          <li>C. Burigana</li>
          <li>C. Colodro-Conde</li>
          <li>D. Di Ferdinando</li>
          <li>R. Farinelli</li>
          <li>J. Graciá-Carpio</li>
          <li>G. Mainetti</li>
          <li>S. Marcin</li>
          <li>N. Mauri</li>
          <li>C. Neissner</li>
          <li>A. A. Nucita</li>
          <li>Z. Sakr</li>
          <li>V. Scottez</li>
          <li>M. Tenti</li>
          <li>M. Viel</li>
          <li>M. Wiesmann</li>
          <li>Y. Akrami</li>
          <li>V. Allevato</li>
          <li>S. Anselmi</li>
          <li>C. Baccigalupi</li>
          <li>M. Ballardini</li>
          <li>S. Borgani</li>
          <li>A. S. Borlaff</li>
          <li>H. Bretonnière</li>
          <li>S. Bruton</li>
          <li>R. Cabanac</li>
          <li>A. Calabro</li>
          <li>A. Cappi</li>
          <li>C. S. Carvalho</li>
          <li>G. Castignani</li>
          <li>T. Castro</li>
          <li>G. Cañas-Herrera</li>
          <li>K. C. Chambers</li>
          <li>J. Coupon</li>
          <li>O. Cucciati</li>
          <li>S. Davini</li>
          <li>G. De Lucia</li>
          <li>G. Desprez</li>
          <li>S. Di Domizio</li>
          <li>H. Dole</li>
          <li>A. Díaz-Sánchez</li>
          <li>J. A. Escartin Vigo</li>
          <li>S. Escoffier</li>
          <li>I. Ferrero</li>
          <li>F. Finelli</li>
          <li>L. Gabarra</li>
          <li>K. Ganga</li>
          <li>J. García-Bellido</li>
          <li>E. Gaztanaga</li>
          <li>K. George</li>
          <li>F. Giacomini</li>
          <li>G. Gozaliasl</li>
          <li>A. Gregorio</li>
          <li>D. Guinet</li>
          <li>A. Hall</li>
          <li>H. Hildebrandt</li>
          <li>A. Jimenez Munoz</li>
          <li>J. J. E. Kajava</li>
          <li>V. Kansal</li>
          <li>D. Karagiannis</li>
          <li>C. C. Kirkpatrick</li>
          <li>L. Legrand</li>
          <li>A. Loureiro</li>
          <li>J. Macias-Perez</li>
          <li>M. Magliocchetti</li>
          <li>R. Maoli</li>
          <li>M. Martinelli</li>
          <li>C. J. A. P. Martins</li>
          <li>S. Matthew</li>
          <li>M. Maturi</li>
          <li>L. Maurin</li>
          <li>R. B. Metcalf</li>
          <li>M. Migliaccio</li>
          <li>P. Monaco</li>
          <li>G. Morgante</li>
          <li>S. Nadathur</li>
          <li>Nicholas A. Walton</li>
          <li>A. Peel</li>
          <li>A. Pezzotta</li>
          <li>V. Popa</li>
          <li>C. Porciani</li>
          <li>D. Potter</li>
          <li>M. Pöntinen</li>
          <li>P. Reimberg</li>
          <li>P. -F. Rocci</li>
          <li>A. G. Sánchez</li>
          <li>A. Schneider</li>
          <li>E. Sefusatti</li>
          <li>M. Sereno</li>
          <li>P. Simon</li>
          <li>A. Spurio Mancini</li>
          <li>S. A. Stanford</li>
          <li>J. Steinwagner</li>
          <li>G. Testera</li>
          <li>M. Tewes</li>
          <li>R. Teyssier</li>
          <li>S. Toft</li>
          <li>S. Tosi</li>
          <li>A. Troja</li>
          <li>M. Tucci</li>
          <li>C. Valieri</li>
          <li>J. Valiviita</li>
          <li>D. Vergani</li>
          <li>I. A. Zinchenko</li>
        </ul>
      </div>
      <div class="pure-g" id="abstract-block">
        <div class="pure-u-1-2">
          <h3>Paper abstract</h3>
          <p>The Euclid mission is expected to image millions of galaxies with high
resolution, providing an extensive dataset to study galaxy evolution. We
investigate the application of deep learning to predict the detailed
morphologies of galaxies in Euclid using Zoobot a convolutional neural network
pretrained with 450000 galaxies from the Galaxy Zoo project. We adapted Zoobot
for emulated Euclid images, generated based on Hubble Space Telescope COSMOS
images, and with labels provided by volunteers in the Galaxy Zoo: Hubble
project. We demonstrate that the trained Zoobot model successfully measures
detailed morphology for emulated Euclid images. It effectively predicts whether
a galaxy has features and identifies and characterises various features such as
spiral arms, clumps, bars, disks, and central bulges. When compared to
volunteer classifications Zoobot achieves mean vote fraction deviations of less
than 12% and an accuracy above 91% for the confident volunteer classifications
across most morphology types. However, the performance varies depending on the
specific morphological class. For the global classes such as disk or smooth
galaxies, the mean deviations are less than 10%, with only 1000 training
galaxies necessary to reach this performance. For more detailed structures and
complex tasks like detecting and counting spiral arms or clumps, the deviations
are slightly higher, around 12% with 60000 galaxies used for training. In order
to enhance the performance on complex morphologies, we anticipate that a larger
pool of labelled galaxies is needed, which could be obtained using
crowdsourcing. Finally, our findings imply that the model can be effectively
adapted to new morphological labels. We demonstrate this adaptability by
applying Zoobot to peculiar galaxies. In summary, our trained Zoobot CNN can
readily predict morphological catalogues for Euclid images.</p>
        </div>
        <div class="pure-u-1-2">
          <h3>LLM summary</h3>
          <p>Q: What is the problem statement of the paper - what are they trying to solve?
A: The paper aims to improve the accuracy of galaxy morphology classification in Euclid by using machine learning algorithms and volunteer labels to overcome the limitations of traditional methods.</p>
          <p>Q: What was the previous state of the art? How did this paper improve upon it?
A: The previous state of the art for galaxy morphology classification in Euclid was based on a set of predefined criteria, such as the presence or absence of certain features like clumps or spiral arms. This paper improved upon these methods by using machine learning algorithms to classify galaxies based on their detailed morphologies and volunteer labels to increase the accuracy of the classification.</p>
          <p>Q: What were the experiments proposed and carried out?
A: The authors proposed two main experiments to evaluate the performance of their machine learning algorithm for galaxy morphology classification. The first experiment used a set of predefined criteria to classify galaxies as clumpy or non-clumpy, while the second experiment used volunteer labels to classify galaxies as symmetric or asymmetric.</p>
          <p>Q: Which figures and tables were referenced in the text most frequently, and/or are the most important for the paper?
A: Figures 1, 2, and 3, and Tables 1 and 2 were referenced in the text most frequently, as they present the results of the experiments proposed in the paper. Figure C.1 shows the distribution of galaxy morphologies in the Euclid survey, while Table 1 provides a summary of the volunteer responses for the clump-count question.</p>
          <p>Q: Which references were cited the most frequently? Under what context were the citations given in?
A: The reference "Kormendy et al. (2019)" was cited the most frequently, as it provides a detailed description of the Euclid survey and its galaxy morphology classification criteria. The authors also cited this reference to highlight the limitations of traditional methods for galaxy morphology classification.</p>
          <p>Q: Why is the paper potentially impactful or important?
A: The paper has the potential to be impactful or important because it proposes a new method for galaxy morphology classification in Euclid that could improve upon the accuracy of traditional methods. By using machine learning algorithms and volunteer labels, the authors demonstrate that it is possible to achieve higher accuracy levels than those obtained with traditional methods.</p>
          <p>Q: What are some of the weaknesses of the paper?
A: The main weakness of the paper is that it relies on a limited number of volunteers to label galaxies, which may not be representative of the larger galaxy population. Additionally, the authors acknowledge that their method may not perform well for galaxies with complex morphologies or those that are difficult to classify.</p>
          <p>Q: What is the Github repository link for this paper?
A: The Github repository link for this paper is not provided in the text.</p>
          <p>Q: Provide up to ten hashtags that describe this paper.
A: #galaxymororphology #Euclid #survey #machinelearning #volunteerlabels #classification #accuracy #limitations #traditionalmethods #newmethod</p>
        </div>
      </div>
    </div>
    <div>
      <h2> 2402.08884v2&mdash;Machine Learning, Density Functional Theory, and Experiments to Understand the Photocatalytic Reduction of CO$_2$ by CuPt/TiO$_2$</h2>
      <p><a href=http://arxiv.org/abs/2402.08884v2>Link to paper</a></p>
      <div id="author-block">
        <ul>
          <li>Vaidish Sumaria</li>
          <li>Takat B. Rawal</li>
          <li>Young Feng Li</li>
          <li>David Sommer</li>
          <li>Jake Vikoren</li>
          <li>Robert J. Bondi</li>
          <li>Matthias Rupp</li>
          <li>Amrit Prasad</li>
          <li>Deeptanshu Prasad</li>
        </ul>
      </div>
      <div class="pure-g" id="abstract-block">
        <div class="pure-u-1-2">
          <h3>Paper abstract</h3>
          <p>The photoconversion of CO$_2$ to hydrocarbons is a sustainable route to its
transformation into value-added compounds and, thereby, crucial to mitigating
the energy and climate crises. CuPt nanoparticles on TiO$_2$ surfaces have been
reported to show promising photoconversion efficiency. For further progress, a
mechanistic understanding of the catalytic properties of these CuPt/TiO$_2$
systems is vital. Here, we employ $\textit{ab-initio}$ calculations, machine
learning, and photocatalysis experiments to explore their configurational space
and examine their reactivity and find that the interface plays a key role in
stabilizing *CO$_2$, *CO, and other CH-containing intermediates, facilitating
higher activity and selectivity for methane. A bias-corrected machine-learning
interatomic potential trained on density functional theory data enables
efficient exploration of the potential energy surfaces of numerous
CO$_2$@CuPt/TiO$_2$ configurations via basin-hopping Monte Carlo simulations,
greatly accelerating the study of these photocatalyst systems. Our simulations
show that CO$_2$ preferentially adsorbs at the interface, with C atom bonded to
a Pt site and one O atom occupying an O-vacancy site. The interface also
promotes the formation of *CH and *CH$_2$ intermediates. For confirmation, we
synthesize CuPt/TiO$_2$ samples with a variety of compositions and analyze
their morphologies and compositions using scanning electron microscopy and
energy-dispersive X-ray spectroscopy, and measure their photocatalytic
activity. Our computational and experimental findings qualitatively agree and
highlight the importance of interface design for selective conversion of CO$_2$
to hydrocarbons.</p>
        </div>
        <div class="pure-u-1-2">
          <h3>LLM summary</h3>
        </div>
      </div>
    </div>
    <div>
      <h2> 2402.08884v2&mdash;Machine Learning, Density Functional Theory, and Experiments to Understand the Photocatalytic Reduction of CO$_2$ by CuPt/TiO$_2$</h2>
      <p><a href=http://arxiv.org/abs/2402.08884v2>Link to paper</a></p>
      <div id="author-block">
        <ul>
          <li>Vaidish Sumaria</li>
          <li>Takat B. Rawal</li>
          <li>Young Feng Li</li>
          <li>David Sommer</li>
          <li>Jake Vikoren</li>
          <li>Robert J. Bondi</li>
          <li>Matthias Rupp</li>
          <li>Amrit Prasad</li>
          <li>Deeptanshu Prasad</li>
        </ul>
      </div>
      <div class="pure-g" id="abstract-block">
        <div class="pure-u-1-2">
          <h3>Paper abstract</h3>
          <p>The photoconversion of CO$_2$ to hydrocarbons is a sustainable route to its
transformation into value-added compounds and, thereby, crucial to mitigating
the energy and climate crises. CuPt nanoparticles on TiO$_2$ surfaces have been
reported to show promising photoconversion efficiency. For further progress, a
mechanistic understanding of the catalytic properties of these CuPt/TiO$_2$
systems is vital. Here, we employ $\textit{ab-initio}$ calculations, machine
learning, and photocatalysis experiments to explore their configurational space
and examine their reactivity and find that the interface plays a key role in
stabilizing *CO$_2$, *CO, and other CH-containing intermediates, facilitating
higher activity and selectivity for methane. A bias-corrected machine-learning
interatomic potential trained on density functional theory data enables
efficient exploration of the potential energy surfaces of numerous
CO$_2$@CuPt/TiO$_2$ configurations via basin-hopping Monte Carlo simulations,
greatly accelerating the study of these photocatalyst systems. Our simulations
show that CO$_2$ preferentially adsorbs at the interface, with C atom bonded to
a Pt site and one O atom occupying an O-vacancy site. The interface also
promotes the formation of *CH and *CH$_2$ intermediates. For confirmation, we
synthesize CuPt/TiO$_2$ samples with a variety of compositions and analyze
their morphologies and compositions using scanning electron microscopy and
energy-dispersive X-ray spectroscopy, and measure their photocatalytic
activity. Our computational and experimental findings qualitatively agree and
highlight the importance of interface design for selective conversion of CO$_2$
to hydrocarbons.</p>
        </div>
        <div class="pure-u-1-2">
          <h3>LLM summary</h3>
        </div>
      </div>
    </div>
    <div>
      <h2> 2402.08734v1&mdash;Passing Stars as an Important Driver of Paleoclimate and the Solar System's Orbital Evolution</h2>
      <p><a href=http://arxiv.org/abs/2402.08734v1>Link to paper</a></p>
      <div id="author-block">
        <ul>
          <li>Nathan A. Kaib</li>
          <li>Sean N. Raymond</li>
        </ul>
      </div>
      <div class="pure-g" id="abstract-block">
        <div class="pure-u-1-2">
          <h3>Paper abstract</h3>
          <p>Reconstructions of the paleoclimate indicate that ancient climatic
fluctuations on Earth are often correlated with variations in its orbital
elements. However, the chaos inherent in the solar system's orbital evolution
prevents numerical simulations from confidently predicting Earth's past orbital
evolution beyond 50-100 Myrs. Gravitational interactions among the Sun's
planets and asteroids are believed to set this limiting time horizon, but most
prior works approximate the solar system as an isolated system and neglect our
surrounding Galaxy. Here we present simulations that include the Sun's nearby
stellar population, and we find that close-passing field stars alter our entire
planetary system's orbital evolution via their gravitational perturbations on
the giant planets. This shortens the timespan over which Earth's orbital
evolution can be definitively known by a further ~10%. In particular, in
simulations that include an exceptionally close passage of the Sun-like star HD
7977 2.8 Myrs ago, new sequences of Earth's orbital evolution become possible
in epochs before ~50 Myrs ago, which includes the Paleocene-Eocene Thermal
Maximum. Thus, simulations predicting Earth's past orbital evolution before ~50
Myrs ago must consider the additional uncertainty from passing stars, which can
open new regimes of past orbital evolution not seen in previous modeling
efforts.</p>
        </div>
        <div class="pure-u-1-2">
          <h3>LLM summary</h3>
        </div>
      </div>
    </div>
    <div>
      <h2> 2402.08734v1&mdash;Passing Stars as an Important Driver of Paleoclimate and the Solar System's Orbital Evolution</h2>
      <p><a href=http://arxiv.org/abs/2402.08734v1>Link to paper</a></p>
      <div id="author-block">
        <ul>
          <li>Nathan A. Kaib</li>
          <li>Sean N. Raymond</li>
        </ul>
      </div>
      <div class="pure-g" id="abstract-block">
        <div class="pure-u-1-2">
          <h3>Paper abstract</h3>
          <p>Reconstructions of the paleoclimate indicate that ancient climatic
fluctuations on Earth are often correlated with variations in its orbital
elements. However, the chaos inherent in the solar system's orbital evolution
prevents numerical simulations from confidently predicting Earth's past orbital
evolution beyond 50-100 Myrs. Gravitational interactions among the Sun's
planets and asteroids are believed to set this limiting time horizon, but most
prior works approximate the solar system as an isolated system and neglect our
surrounding Galaxy. Here we present simulations that include the Sun's nearby
stellar population, and we find that close-passing field stars alter our entire
planetary system's orbital evolution via their gravitational perturbations on
the giant planets. This shortens the timespan over which Earth's orbital
evolution can be definitively known by a further ~10%. In particular, in
simulations that include an exceptionally close passage of the Sun-like star HD
7977 2.8 Myrs ago, new sequences of Earth's orbital evolution become possible
in epochs before ~50 Myrs ago, which includes the Paleocene-Eocene Thermal
Maximum. Thus, simulations predicting Earth's past orbital evolution before ~50
Myrs ago must consider the additional uncertainty from passing stars, which can
open new regimes of past orbital evolution not seen in previous modeling
efforts.</p>
        </div>
        <div class="pure-u-1-2">
          <h3>LLM summary</h3>
        </div>
      </div>
    </div>
</body>
</html>