<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>2023&mdash;6 summaries</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/purecss@3.0.0/build/pure-min.css" integrity="sha384-X38yfunGUhNzHpBaEBsWLO+A0HDYOQi8ufWDkZ0k9e0eXz/tH3II7uKZ9msv++Ls" crossorigin="anonymous">
    <script type="text/javascript" id="MathJax-script" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.0.0/es5/latest?tex-mml-chtml.js">
    </script>
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        }
      };
    </script>
    <script id="MathJax-script" async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
    </script>
    <link rel="stylesheet" href="../../../style.css">
</head>
<body>
    <h1>Summaries for 2023/6</h1>
    <hr>
    <p>
      <em class="disclaimer">Disclaimer: summary content on this page has been generated using a LLM with RAG, and may not have been checked for factual accuracy. The human-written abstract is provided alongside each summary.</em>
    </p>
    <div>
      <h2> 2306.12059v3&mdash;EquiformerV2: Improved Equivariant Transformer for Scaling to Higher-Degree Representations</h2>
      <p><a href=http://arxiv.org/abs/2306.12059v3>Link to paper</a></p>
      <div id="author-block">
        <ul>
          <li>Yi-Lun Liao</li>
          <li>Brandon Wood</li>
          <li>Abhishek Das</li>
          <li>Tess Smidt</li>
        </ul>
      </div>
      <div class="pure-g" id="abstract-block">
        <div class="pure-u-1-2">
          <h3>Paper abstract</h3>
          <p>Equivariant Transformers such as Equiformer have demonstrated the efficacy of
applying Transformers to the domain of 3D atomistic systems. However, they are
limited to small degrees of equivariant representations due to their
computational complexity. In this paper, we investigate whether these
architectures can scale well to higher degrees. Starting from Equiformer, we
first replace $SO(3)$ convolutions with eSCN convolutions to efficiently
incorporate higher-degree tensors. Then, to better leverage the power of higher
degrees, we propose three architectural improvements -- attention
re-normalization, separable $S^2$ activation and separable layer normalization.
Putting this all together, we propose EquiformerV2, which outperforms previous
state-of-the-art methods on large-scale OC20 dataset by up to $9\%$ on forces,
$4\%$ on energies, offers better speed-accuracy trade-offs, and $2\times$
reduction in DFT calculations needed for computing adsorption energies.
Additionally, EquiformerV2 trained on only OC22 dataset outperforms GemNet-OC
trained on both OC20 and OC22 datasets, achieving much better data efficiency.
Finally, we compare EquiformerV2 with Equiformer on QM9 and OC20 S2EF-2M
datasets to better understand the performance gain brought by higher degrees.</p>
        </div>
        <div class="pure-u-1-2">
          <h3>LLM summary</h3>
          <p>Q: What is the problem statement of the paper - what are they trying to solve?
A: The authors aim to improve upon the previous state of the art in terms of predicting the energy of an adsorbate on a catalyst surface using machine learning. They identify that the current methods have limitations in terms of accuracy and computational cost, and thus propose new architectural improvements to address these issues.</p>
          <p>Q: What was the previous state of the art? How did this paper improve upon it?
A: The previous state of the art in terms of predicting the energy of an adsorbate on a catalyst surface using machine learning was the EquiformerV2 model proposed by Liao and Smidt (2023). The current paper improves upon this model by introducing new architectural improvements, such as increasing the maximum sequence length (Lmax), using eSCN convolutions, attention re-normalization, normalization of the input, and separable layer normalization. These improvements lead to improved performance in terms of lower MAE values compared to the EquiformerV2 model.</p>
          <p>Q: What were the experiments proposed and carried out?
A: The authors conducted ablation studies on the proposed architectural improvements using the task of ∆ε on the QM9 dataset. They evaluated the performance of the different variants of the model and identified which improvements led to better results.</p>
          <p>Q: Which figures and tables were referenced in the text most frequently, and/or are the most important for the paper?
A: Figures 6 and Table 14 were referenced the most frequently in the text. Figure 6 shows qualitative examples of the initial configuration of an adsorbate on a catalyst surface and the corresponding relaxed configurations obtained from different models, while Table 14 presents the results of ablation studies on the proposed architectural improvements.</p>
          <p>Q: Which references were cited the most frequently? Under what context were the citations given in?
A: The reference cited the most frequently is Liao and Smidt (2023), who proposed the EquiformerV2 model. The authors mention this reference in the context of comparing their proposed improvements to the state of the art, highlighting the importance of their work in terms of improving machine learning models for predicting adsorbate energy on catalyst surfaces.</p>
          <p>Q: Why is the paper potentially impactful or important?
A: The authors argue that their proposed model has the potential to be impactful due to its improved accuracy and reduced computational cost compared to previous methods. This could lead to faster and more accurate simulations of adsorption processes, which are essential for optimizing catalyst design and predicting material properties.</p>
          <p>Q: What are some of the weaknesses of the paper?
A: The authors acknowledge that their proposed model is computationally expensive due to the use of eSCN convolutions, which could be a limitation for large-scale simulations. Additionally, they mention that the QM9 dataset has limitations in terms of size and diversity, which could impact the generalizability of their results to other datasets or systems.</p>
          <p>Q: Is a link to the Github code provided? If there isn't or you are unsure, say you don't know.
A: No link to the Github code is provided in the paper.</p>
          <p>Q: Provide up to ten hashtags that describe this paper.
A: #machinelearning #adsorption #catalyst #surfacechemistry #computationalmaterialscience #accurateprediction #efficientsimulation #materialsdesign #optimaldesign #accuracyimprovement #computationalcostreduction</p>
        </div>
      </div>
    </div>
</body>
</html>