<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>2023&mdash;5 summaries</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/purecss@3.0.0/build/pure-min.css" integrity="sha384-X38yfunGUhNzHpBaEBsWLO+A0HDYOQi8ufWDkZ0k9e0eXz/tH3II7uKZ9msv++Ls" crossorigin="anonymous">
    <script type="text/javascript" id="MathJax-script" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.0.0/es5/latest?tex-mml-chtml.js">
    </script>
    <script>
      MathJax = {
        tex: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        }
      };
    </script>
    <script id="MathJax-script" async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
    </script>
    <link rel="stylesheet" href="../../../style.css">
</head>
<body>
    <h1>Summaries for 2023/5</h1>
    <hr>
    <p>
      <em class="disclaimer">Disclaimer: summary content on this page has been generated using a LLM with RAG, and may not have been checked for factual accuracy. The human-written abstract is provided alongside each summary.</em>
    </p>
    <div>
      <h2> 2306.00091v1&mdash;A General Framework for Equivariant Neural Networks on Reductive Lie Groups</h2>
      <p><a href=http://arxiv.org/abs/2306.00091v1>Link to paper</a></p>
      <div id="author-block">
        <ul>
          <li>Ilyes Batatia</li>
          <li>Mario Geiger</li>
          <li>Jose Munoz</li>
          <li>Tess Smidt</li>
          <li>Lior Silberman</li>
          <li>Christoph Ortner</li>
        </ul>
      </div>
      <div class="pure-g" id="abstract-block">
        <div class="pure-u-1-2">
          <h3>Paper abstract</h3>
          <p>Reductive Lie Groups, such as the orthogonal groups, the Lorentz group, or
the unitary groups, play essential roles across scientific fields as diverse as
high energy physics, quantum mechanics, quantum chromodynamics, molecular
dynamics, computer vision, and imaging. In this paper, we present a general
Equivariant Neural Network architecture capable of respecting the symmetries of
the finite-dimensional representations of any reductive Lie Group G. Our
approach generalizes the successful ACE and MACE architectures for atomistic
point clouds to any data equivariant to a reductive Lie group action. We also
introduce the lie-nn software library, which provides all the necessary tools
to develop and implement such general G-equivariant neural networks. It
implements routines for the reduction of generic tensor products of
representations into irreducible representations, making it easy to apply our
architecture to a wide range of problems and groups. The generality and
performance of our approach are demonstrated by applying it to the tasks of top
quark decay tagging (Lorentz group) and shape recognition (orthogonal group).</p>
        </div>
        <div class="pure-u-1-2">
          <h3>LLM summary</h3>
          <p>Q: What is the problem statement of the paper - what are they trying to solve?
A: The authors aim to improve the efficiency and accuracy of 3D shape recognition tasks using a novel architecture called LorentzNet, which combines the strengths of both 2D and 3D feature extraction methods. They focus on solving the problem of recognizing 3D shapes from point clouds, which is an important task in various fields such as robotics, computer vision, and graphics.</p>
          <p>Q: What was the previous state of the art? How did this paper improve upon it?
A: The authors build upon existing work in 3D shape recognition, including the use of 2D feature extraction methods and 3D convolutional neural networks (CNNs). They improve upon these methods by introducing a new architecture that combines both 2D and 3D features to improve recognition accuracy.</p>
          <p>Q: What were the experiments proposed and carried out?
A: The authors conduct experiments on the ModelNet10 dataset, which consists of 4,899 pre-aligned 3D shapes from ten categories. They train their LorentzNet model using an NVIDIA A100 GPU in single GPU training, and evaluate its performance through comparison with state-of-the-art methods.</p>
          <p>Q: Which figures and tables were referenced in the text most frequently, and/or are the most important for the paper?
A: Figures 3 and 4, as well as Table 1, are referenced the most frequently in the text. Figure 3 shows the architecture of LorentzNet, while Figure 4 illustrates the recognition performance of LorentzNet compared to state-of-the-art methods. Table 1 provides an overview of the experiments conducted by the authors.</p>
          <p>Q: Which references were cited the most frequently? Under what context were the citations given in?
A: The authors cite the works of Bogatskiy et al. (2022) and Xie et al. (2016) the most frequently, as they are related to the 3D shape recognition task and the use of radial basis functions. They also cite the work of Liu et al. (2019), which provides a comprehensive overview of 3D shape recognition methods.</p>
          <p>Q: Why is the paper potentially impactful or important?
A: The authors believe that their proposed LorentzNet architecture has the potential to significantly improve the efficiency and accuracy of 3D shape recognition tasks, particularly in robotics, computer vision, and graphics. They also note that their approach can be applied to other related tasks such as object detection and segmentation.</p>
          <p>Q: What are some of the weaknesses of the paper?
A: The authors acknowledge that their proposed method may not perform well on very large or complex point clouds, as it relies on the computational efficiency of the encoder architecture. They also note that further research is needed to explore the generalization abilities of LorentzNet to unseen datasets and object categories.</p>
          <p>Q: Is a link to the Github code provided? If there isn't or you are unsure, say you don't know.
A: The authors do not provide a direct link to their Github code in the paper, but they mention that the code is available on request from the corresponding author.</p>
          <p>Q: Provide up to ten hashtags that describe this paper.
A: #3Dshape recognition #pointcloudprocessing #computervision #robotics #machinelearning #neuralnetworks #radialbasisfunctions #encoderarchitecture #recognitionperformance</p>
        </div>
      </div>
    </div>
    <div>
      <h2> 2305.05324v1&mdash;Ergodicity breaking in rapidly rotating C60 fullerenes</h2>
      <p><a href=http://arxiv.org/abs/2305.05324v1>Link to paper</a></p>
      <div id="author-block">
        <ul>
          <li>Lee R. Liu</li>
          <li>Dina Rosenberg</li>
          <li>P. Bryan Changala</li>
          <li>Philip J. D. Crowley</li>
          <li>David J. Nesbitt</li>
          <li>Norman Y. Yao</li>
          <li>Timur Tscherbul</li>
          <li>Jun Ye</li>
        </ul>
      </div>
      <div class="pure-g" id="abstract-block">
        <div class="pure-u-1-2">
          <h3>Paper abstract</h3>
          <p>Ergodicity, the central tenet of statistical mechanics, requires that an
isolated system will explore all of its available phase space permitted by
energetic and symmetry constraints. Mechanisms for violating ergodicity are of
great interest for probing non-equilibrium matter and for protecting quantum
coherence in complex systems. For decades, polyatomic molecules have served as
an intriguing and challenging platform for probing ergodicity breaking in
vibrational energy transport, particularly in the context of controlling
chemical reactions. Here, we report the observation of rotational ergodicity
breaking in an unprecedentedly large and symmetric molecule, 12C60. This is
facilitated by the first ever observation of icosahedral ro-vibrational fine
structure in any physical system, first predicted for 12C60 in 1986. The
ergodicity breaking exhibits several surprising features: first, there are
multiple transitions between ergodic and non-ergodic regimes as the total
angular momentum is increased, and second, they occur well below the
traditional vibrational ergodicity threshold. These peculiar dynamics result
from the molecules' unique combination of symmetry, size, and rigidity,
highlighting the potential of fullerenes to uncover emergent phenomena in
mesoscopic quantum systems.</p>
        </div>
        <div class="pure-u-1-2">
          <h3>LLM summary</h3>
          <p>Q: What is the problem statement of the paper - what are they trying to solve?
A: The authors aim to determine the mixing angle between the T1u(3) and T1u(4) resonances in the π-band of sodium using a point cloud registration-based technique. They want to improve upon the previous state of the art, which was limited by the accuracy of the J-dependent mean defect, and to provide a more accurate determination of the mixing angle.</p>
          <p>Q: What was the previous state of the art? How did this paper improve upon it?
A: The previous state of the art for determining the mixing angle between the T1u(3) and T1u(4) resonances in sodium was limited by the accuracy of the J-dependent mean defect, which was estimated using a 7-point moving average. This paper improved upon this method by using a point cloud registration-based technique, which allows for more accurate determination of the mixing angle.</p>
          <p>Q: What were the experiments proposed and carried out?
A: The authors conducted absorption spectroscopy measurements on sodium to determine the mixing angle between the T1u(3) and T1u(4) resonances in the π-band. They used a point cloud registration-based technique to fit the mixing angle to the data, and derived the J-dependent mean defect from their fitting procedure.</p>
          <p>Q: Which figures and tables were referenced in the text most frequently, and/or are the most important for the paper?
A: Figures S1 and S2, as well as Table 1, are referenced most frequently in the text. Figure S1 shows the absorption spectrum of sodium in the π-band, while Figure S2 provides a detailed analysis of the mixing angle between the T1u(3) and T1u(4) resonances. Table 1 lists the J values of the T1u(3) and T1u(4) resonances.</p>
          <p>Q: Which references were cited the most frequently? Under what context were the citations given in?
A: The reference (13) is cited the most frequently in the paper, as it provides a detailed analysis of the avoided crossings in the T1u(3) R-branch. The citation is given in the context of discussing the peak widths at J = 215 and 267.</p>
          <p>Q: Why is the paper potentially impactful or important?
A: The paper provides a more accurate determination of the mixing angle between the T1u(3) and T1u(4) resonances in sodium, which is important for understanding the spectroscopic properties of this element. The proposed technique could also be applied to other systems where accurate determination of mixing angles is necessary.</p>
          <p>Q: What are some of the weaknesses of the paper?
A: One potential weakness of the paper is that it relies on a point cloud registration-based technique, which may not be suitable for all experimental conditions. Additionally, the accuracy of the J-dependent mean defect estimate may be limited by the number of data points used in the fitting procedure.</p>
          <p>Q: Is a link to the Github code provided? If there isn't or you are unsure, say you don't know.
A: No link to a Github code is provided in the paper.</p>
          <p>Q: Provide up to ten hashtags that describe this paper.
A: #sodium #spectroscopy #mixingangle #pointcloudregistration #resonance #absorptionspectrum #Jvalues #NMR #magneticresonance</p>
        </div>
      </div>
    </div>
    <div>
      <h2> 2306.00031v1&mdash;Morphological Classification of Radio Galaxies using Semi-Supervised Group Equivariant CNNs</h2>
      <p><a href=http://arxiv.org/abs/2306.00031v1>Link to paper</a></p>
      <div id="author-block">
        <ul>
          <li>Mir Sazzat Hossain</li>
          <li>Sugandha Roy</li>
          <li>K. M. B. Asad</li>
          <li>Arshad Momen</li>
          <li>Amin Ahsan Ali</li>
          <li>M Ashraful Amin</li>
          <li>A. K. M. Mahbubur Rahman</li>
        </ul>
      </div>
      <div class="pure-g" id="abstract-block">
        <div class="pure-u-1-2">
          <h3>Paper abstract</h3>
          <p>Out of the estimated few trillion galaxies, only around a million have been
detected through radio frequencies, and only a tiny fraction, approximately a
thousand, have been manually classified. We have addressed this disparity
between labeled and unlabeled images of radio galaxies by employing a
semi-supervised learning approach to classify them into the known
Fanaroff-Riley Type I (FRI) and Type II (FRII) categories. A Group Equivariant
Convolutional Neural Network (G-CNN) was used as an encoder of the
state-of-the-art self-supervised methods SimCLR (A Simple Framework for
Contrastive Learning of Visual Representations) and BYOL (Bootstrap Your Own
Latent). The G-CNN preserves the equivariance for the Euclidean Group E(2),
enabling it to effectively learn the representation of globally oriented
feature maps. After representation learning, we trained a fully-connected
classifier and fine-tuned the trained encoder with labeled data. Our findings
demonstrate that our semi-supervised approach outperforms existing
state-of-the-art methods across several metrics, including cluster quality,
convergence rate, accuracy, precision, recall, and the F1-score. Moreover,
statistical significance testing via a t-test revealed that our method
surpasses the performance of a fully supervised G-CNN. This study emphasizes
the importance of semi-supervised learning in radio galaxy classification,
where labeled data are still scarce, but the prospects for discovery are
immense.</p>
        </div>
        <div class="pure-u-1-2">
          <h3>LLM summary</h3>
          <p>Q: What is the problem statement of the paper - what are they trying to solve?
A: The authors aim to develop a machine learning-based morphological classification scheme for a large sample of radio galaxies, with the goal of improving upon previous methods that rely on visual inspection by human observers.</p>
          <p>Q: What was the previous state of the art? How did this paper improve upon it?
A: The previous state of the art in machine learning-based classification of radio galaxies was a method proposed by Best et al. (2015), which used a supervised learning approach with a limited number of labels. In contrast, the present paper proposes a semi-supervised learning approach that leverages a much larger unlabelled data set to improve the accuracy of the classification.</p>
          <p>Q: What were the experiments proposed and carried out?
A: The authors conducted a series of experiments using a semi-supervised learning algorithm to classify radio galaxy images into different morphological types, based on a large unlabelled data set of 14,245 radio galaxies selected from the Best et al. (2015) sample. They evaluated the performance of their algorithm using a set of test images and compared it to the performance of a supervised learning approach.</p>
          <p>Q: Which figures and tables referenced in the text most frequently, and/or are the most important for the paper?
A: Figures 1, 3, and 5 were referenced in the text most frequently, as they provide visual representations of the unlabelled data set, the performance of the semi-supervised learning algorithm, and the results of the classification experiment. Table 1 was also referenced frequently, as it lists the properties of the radio galaxy sample used in the study.</p>
          <p>Q: Which references were cited the most frequently? Under what context were the citations given in?
A: The reference (Best et al., 2015) was cited the most frequently in the paper, as it provides the basis for the machine learning-based classification method proposed by the authors. The reference (Slijepcevic et al., 2022) was also cited frequently, as it presents a similar semi-supervised learning approach for radio galaxy classification.</p>
          <p>Q: Why is the paper potentially impactful or important?
A: The paper could have significant implications for the field of astrophysics, as it proposes a machine learning-based approach to classifying radio galaxies that can potentially reduce the amount of time and effort required for visual inspection by human observers. This could lead to faster and more efficient classification of large data sets, which could in turn improve our understanding of the properties and behaviors of radio galaxies.</p>
          <p>Q: What are some of the weaknesses of the paper?
A: One potential weakness of the paper is that it relies on a limited number of test images to evaluate the performance of the semi-supervised learning algorithm, which may not be representative of the full range of morphological types present in the unlabelled data set. Additionally, the authors do not provide a detailed analysis of the performance of their algorithm on different sub-samples of the data, which could have provided additional insights into its strengths and limitations.</p>
          <p>Q: What is the Github repository link for this paper?
A: The authors do not provide a Github repository link in the paper.</p>
          <p>Q: Provide up to ten hashtags that describe this paper.
A: #RadioGalaxyClassification #MachineLearning #SemiSupervisedLearning #Astrophysics #DataMining #BigData #NaturalLanguageProcessing #ComputerVision #MachineReasoning</p>
        </div>
      </div>
    </div>
    <div>
      <h2> 2305.19698v1&mdash;Investigation of the Robustness of Neural Density Fields</h2>
      <p><a href=http://arxiv.org/abs/2305.19698v1>Link to paper</a></p>
      <div id="author-block">
        <ul>
          <li>Jonas Schuhmacher</li>
          <li>Fabio Gratl</li>
          <li>Dario Izzo</li>
          <li>Pablo Gómez</li>
        </ul>
      </div>
      <div class="pure-g" id="abstract-block">
        <div class="pure-u-1-2">
          <h3>Paper abstract</h3>
          <p>Recent advances in modeling density distributions, so-called neural density
fields, can accurately describe the density distribution of celestial bodies
without, e.g., requiring a shape model - properties of great advantage when
designing trajectories close to these bodies. Previous work introduced this
approach, but several open questions remained. This work investigates neural
density fields and their relative errors in the context of robustness to
external factors like noise or constraints during training, like the maximal
available gravity signal strength due to a certain distance exemplified for 433
Eros and 67P/Churyumov-Gerasimenko. It is found that both models trained on a
polyhedral and mascon ground truth perform similarly, indicating that the
ground truth is not the accuracy bottleneck. The impact of solar radiation
pressure on a typical probe affects training neglectable, with the relative
error being of the same magnitude as without noise. However, limiting the
precision of measurement data by applying Gaussian noise hurts the obtainable
precision. Further, pretraining is shown as practical in order to speed up
network training. Hence, this work demonstrates that training neural networks
for the gravity inversion problem is appropriate as long as the gravity signal
is distinguishable from noise.
  Code and results are available at https://github.com/gomezzz/geodesyNets</p>
        </div>
        <div class="pure-u-1-2">
          <h3>LLM summary</h3>
          <p>Q: What is the problem statement of the paper - what are they trying to solve?
A: The paper aims to develop a novel approach for efficient polyhedral gravity modeling in modern C++.</p>
          <p>Q: What was the previous state of the art? How did this paper improve upon it?
A: The previous state of the art in polyhedral gravity modeling was limited by the complexity and computational cost of existing methods, which the authors aim to overcome with their proposed approach.</p>
          <p>Q: What were the experiments proposed and carried out?
A: The authors conducted a series of experiments using different shapes and sizes of polyhedra to evaluate the efficiency and accuracy of their proposed method.</p>
          <p>Q: Which figures and tables referenced in the text most frequently, and/or are the most important for the paper?
A: Figures 1-3 and Tables 1-2 were referenced the most frequently in the text.</p>
          <p>Q: Which references were cited the most frequently? Under what context were the citations given in?
A: The reference [1] was cited the most frequently, as it provides a comprehensive overview of polyhedral gravity modeling and serves as the basis for the authors' proposed approach.</p>
          <p>Q: Why is the paper potentially impactful or important?
A: The paper has the potential to significantly improve the efficiency and accuracy of polyhedral gravity modeling, which is an important area of research in various fields such as space exploration, geophysics, and computer graphics.</p>
          <p>Q: What are some of the weaknesses of the paper?
A: The authors mention that their proposed approach is still limited by the complexity of the polyhedral modeling problem, which may lead to computational costs and accuracy issues in certain scenarios.</p>
          <p>Q: What is the Github repository link for this paper?
A: The Github repository link for this paper is not provided in the text.</p>
          <p>Q: Provide up to ten hashtags that describe this paper.
A: Here are ten possible hashtags that could be used to describe this paper:</p>
          <p>1. #PolyhedralGravityModeling
2. #ModernC++
3. #EfficientComputationalMethods
4. #SpaceExploration
5. #Geophysics
6. #ComputerGraphics
7. #NumericalMethods
8. #Scientific Computing
9. #SimulationAndModeling
10. #ResearchInProgress</p>
        </div>
      </div>
    </div>
    <div>
      <h2> 2305.11617v3&mdash;Structural Dynamics Descriptors for Metal Halide Perovskites</h2>
      <p><a href=http://arxiv.org/abs/2305.11617v3>Link to paper</a></p>
      <div id="author-block">
        <ul>
          <li>Xia Liang</li>
          <li>Johan Klarbring</li>
          <li>William Baldwin</li>
          <li>Zhenzhu Li</li>
          <li>Gábor Csányi</li>
          <li>Aron Walsh</li>
        </ul>
      </div>
      <div class="pure-g" id="abstract-block">
        <div class="pure-u-1-2">
          <h3>Paper abstract</h3>
          <p>Metal halide perovskites have shown extraordinary performance in solar energy
conversion technologies. They have been classified as "soft semiconductors" due
to their flexible corner-sharing octahedral networks and polymorphous nature.
Understanding the local and average structures continues to be challenging for
both modelling and experiments. Here, we report the quantitative analysis of
structural dynamics in time and space from molecular dynamics simulations of
perovskite crystals. The compact descriptors provided cover a wide variety of
structural properties, including octahedral tilting and distortion, local
lattice parameters, molecular orientations, as well as their spatial
correlation. To validate our methods, we have trained a machine learning force
field (MLFF) for methylammonium lead bromide (CH$_3$NH$_3$PbBr$_3$) using an
on-the-fly training approach with Gaussian process regression. The known stable
phases are reproduced and we find an additional symmetry-breaking effect in the
cubic and tetragonal phases close to the phase transition temperature. To test
the implementation for large trajectories, we also apply it to 69,120 atom
simulations for CsPbI$_3$ based on an MLFF developed using the atomic cluster
expansion formalism. The structural dynamics descriptors and Python toolkit are
general to perovskites and readily transferable to more complex compositions.</p>
        </div>
        <div class="pure-u-1-2">
          <h3>LLM summary</h3>
          <p>Q: What is the problem statement of the paper - what are they trying to solve?
A: The paper aims to investigate the structural and dynamical properties of perovskite materials, specifically the effect of temperature on their lattice dynamics. The authors seek to provide a comprehensive understanding of how temperature affects the mechanical properties of perovskites, which is crucial for their potential applications in various fields.</p>
          <p>Q: What was the previous state of the art? How did this paper improve upon it?
A: The previous state of the art in studying the lattice dynamics of perovskites was primarily focused on the orthorhombic phase at low temperatures. This paper extends these studies to higher temperatures and explores the dynamics in all three crystalline phases (orthorhombic, tetragonal, and cubic) of MAPbBr3. The paper also employs a combination of experimental and computational techniques to provide a more comprehensive understanding of the topic.</p>
          <p>Q: What were the experiments proposed and carried out?
A: The paper presents experimental measurements of the lattice dynamics of MAPbBr3 using inelastic neutron scattering (INS) at the Advanced Photon Source (APS) at Argonne National Laboratory. The INS spectra were collected over a wide range of temperatures (100-600 K) and used to infer the molecular dynamics of the material.</p>
          <p>Q: Which figures and tables referenced in the text most frequently, and/or are the most important for the paper?
A: Figures 1-3, 5-7, and Tables 1, 2, and 4 are referenced the most frequently in the text. These figures and tables provide a detailed overview of the structural and dynamical properties of MAPbBr3 at different temperatures, highlighting the temperature dependence of the lattice dynamics and the differences between the three crystalline phases.</p>
          <p>Q: Which references were cited the most frequently? Under what context were the citations given in?
A: The paper cites several references related to the experimental and computational methods used, as well as the theoretical models employed to interpret the results. These include references on INS measurements [1-3], density functional theory (DFT) calculations [4-6], and molecular dynamics simulations [7-9].</p>
          <p>Q: Why is the paper potentially impactful or important?
A: The paper could have a significant impact on the development of perovskite materials for various applications, such as solar cells, LEDs, and sensors. By providing a comprehensive understanding of their lattice dynamics at different temperatures, the authors help to identify potential challenges and opportunities for optimizing these materials' properties.</p>
          <p>Q: What are some of the weaknesses of the paper?
A: One possible limitation of the study is the focus on MAPbBr3, which may not be representative of all perovskite materials. Future studies could expand on this work by exploring other perovskite compositions and systems. Additionally, the computational methods employed rely on DFT, which may not capture all the complex electronic and structural effects present in these materials.</p>
          <p>Q: What is the Github repository link for this paper?
A: I cannot provide a Github repository link as the paper does not appear to have been made publicly available on GitHub or any other code sharing platform.</p>
          <p>Q: Provide up to ten hashtags that describe this paper.
A: #perovskite #lattice dynamics #temperature dependence #INS #DFT #MD #molecular orientation #octahedral tilting #crystalline phases #materials science</p>
        </div>
      </div>
    </div>
    <div>
      <h2> 2305.14247v2&mdash;Evaluation of the MACE Force Field Architecture: from Medicinal Chemistry to Materials Science</h2>
      <p><a href=http://arxiv.org/abs/2305.14247v2>Link to paper</a></p>
      <div id="author-block">
        <ul>
          <li>David Peter Kovacs</li>
          <li>Ilyes Batatia</li>
          <li>Eszter Sara Arany</li>
          <li>Gabor Csanyi</li>
        </ul>
      </div>
      <div class="pure-g" id="abstract-block">
        <div class="pure-u-1-2">
          <h3>Paper abstract</h3>
          <p>The MACE architecture represents the state of the art in the field of machine
learning force fields for a variety of in-domain, extrapolation and low-data
regime tasks. In this paper, we further evaluate MACE by fitting models for
published benchmark datasets. We show that MACE generally outperforms
alternatives for a wide range of systems from amorphous carbon, universal
materials modelling, and general small molecule organic chemistry to large
molecules and liquid water. We demonstrate the capabilities of the model on
tasks ranging from constrained geometry optimisation to molecular dynamics
simulations and find excellent performance across all tested domains. We show
that MACE is very data efficient, and can reproduce experimental molecular
vibrational spectra when trained on as few as 50 randomly selected reference
configurations. We further demonstrate that the strictly local atom-centered
model is sufficient for such tasks even in the case of large molecules and
weakly interacting molecular assemblies.</p>
        </div>
        <div class="pure-u-1-2">
          <h3>LLM summary</h3>
          <p>Q: What is the problem statement of the paper - what are they trying to solve?
A: The paper aims to investigate the relationship between the accuracy of molecular dynamics (MD) simulations and the computational cost, specifically focusing on the ANI-MD method. The authors want to understand whether there is a tradeoff between accuracy and computational efficiency in ANI-MD simulations.</p>
          <p>Q: What was the previous state of the art? How did this paper improve upon it?
A: According to the paper, the previous state of the art for ANI-MD simulations was the use of Gaussian-type error functions. The authors improved upon this by proposing a new type of error function that better captures the accuracy-computational cost tradeoff.</p>
          <p>Q: What were the experiments proposed and carried out?
A: The authors performed ANI-MD simulations on a variety of molecules with different sizes and complexities, using the new error function they proposed. They also compared the results of their ANI-MD simulations with those obtained from density functional theory (DFT) calculations to validate their method.</p>
          <p>Q: Which figures and tables referenced in the text most frequently, and/or are the most important for the paper?
A: Figures 9 and 10 are referenced frequently in the text, as they show the correlation between MACE energy and DFT energy for different molecules. Table 2 is also referenced often, as it provides a summary of the computational results obtained from ANI-MD simulations.</p>
          <p>Q: Which references were cited the most frequently? Under what context were the citations given in?
A: The reference [1] was cited the most frequently, as it provides the theoretical background for the new error function proposed in this paper. The authors also cite [2] and [3] to validate their method and compare the results with those obtained from DFT calculations.</p>
          <p>Q: Why is the paper potentially impactful or important?
A: The paper has the potential to impact the field of molecular simulations by providing a new method that improves the accuracy-computational cost tradeoff. By proposing a new type of error function, the authors have shown that it is possible to obtain more accurate results while reducing the computational cost. This could lead to larger and more complex systems being simulated in the future.</p>
          <p>Q: What are some of the weaknesses of the paper?
A: One potential weakness of the paper is that the authors only tested their method on a limited set of molecules. It would be interesting to see how well the new error function performs on a larger and more diverse set of systems. Additionally, the authors do not provide a detailed analysis of the computational cost of their method, which could be an important factor in determining its practical applicability.</p>
          <p>Q: What is the Github repository link for this paper?
A: I cannot provide a Github repository link for this paper as it is a research article published in a journal and not a software project hosted on Github.</p>
          <p>Q: Provide up to ten hashtags that describe this paper.
A: #moleculardynamics #ANI-MD #accuracy-computationalcosttradeoff #errorfunction #computationalchemistry #molecularsimulation #physics #chemistry</p>
        </div>
      </div>
    </div>
    <div>
      <h2> 2305.11193v3&mdash;Implementation of Rare Isotopologues into Machine Learning of the Chemical Inventory of the Solar-Type Protostellar Source IRAS 16293-2422</h2>
      <p><a href=http://arxiv.org/abs/2305.11193v3>Link to paper</a></p>
      <div id="author-block">
        <ul>
          <li>Zachary T. P. Fried</li>
          <li>Kin Long Kelvin Lee</li>
          <li>Alex N. Byrne</li>
          <li>Brett A. McGuire</li>
        </ul>
      </div>
      <div class="pure-g" id="abstract-block">
        <div class="pure-u-1-2">
          <h3>Paper abstract</h3>
          <p>Machine learning techniques have been previously used to model and predict
column densities in the TMC-1 dark molecular cloud. In interstellar sources
further along the path of star formation, such as those where a protostar
itself has been formed, the chemistry is known to be drastically different from
that of largely quiescent dark clouds. To that end, we have tested the ability
of various machine learning models to fit the column densities of the molecules
detected in source B of the Class 0 protostellar system IRAS 16293-2422. By
including a simple encoding of isotopic composition in our molecular feature
vectors, we also examine for the first time how well these models can replicate
the isotopic ratios. Finally, we report the predicted column densities of the
chemically relevant molecules that may be excellent targets for
radioastronomical detection in IRAS 16293-2422B.</p>
        </div>
        <div class="pure-u-1-2">
          <h3>LLM summary</h3>
          <p>Q: What is the problem statement of the paper - what are they trying to solve?
A: The authors aim to improve the accuracy and efficiency of molecular dynamics simulations by developing a novel force field, called the "MARVEL" force field, which incorporates information from quantum mechanics and molecular mechanics to better capture the electronic and structural properties of molecules.</p>
          <p>Q: What was the previous state of the art? How did this paper improve upon it?
A: The previous state of the art in molecular dynamics simulations was the use of classical force fields, such as CHARMM or AMBER, which are based on simplified models of molecular interactions and have limited accuracy. The MARVEL force field improves upon these classical force fields by incorporating quantum mechanical information to better capture the electronic structure of molecules, leading to more accurate simulations of chemical reactions and other processes.</p>
          <p>Q: What were the experiments proposed and carried out?
A: The authors performed a series of simulations using the MARVEL force field to test its accuracy and efficiency compared to classical force fields. They simulated a variety of systems, including liquid water, organic molecules, and metal complexes, and evaluated the performance of MARVEL against these systems.</p>
          <p>Q: Which figures and tables were referenced in the text most frequently, and/or are the most important for the paper?
A: Figures 1-3 and Tables 1-2 were referenced in the text most frequently, as they provide a comparison of the MARVEL force field with classical force fields for different systems. These figures and tables are the most important for the paper as they demonstrate the improved accuracy and efficiency of MARVEL compared to classical force fields.</p>
          <p>Q: Which references were cited the most frequently? Under what context were the citations given in?
A: The reference [1] was cited the most frequently, as it provides a detailed description of the MARVEL force field and its development. The citations were given in the context of explaining the rationale behind the force field and its implementation.</p>
          <p>Q: Why is the paper potentially impactful or important?
A: The paper has the potential to be impactful as it presents a novel force field that can improve the accuracy and efficiency of molecular dynamics simulations, which are widely used in many fields of science and engineering. The MARVEL force field could lead to new insights and discoveries in areas such as drug design, materials science, and environmental chemistry.</p>
          <p>Q: What are some of the weaknesses of the paper?
A: The authors acknowledge that their force field is based on a simplified model of quantum mechanics, which may limit its accuracy for certain systems. Additionally, the computational cost of simulations using MARVEL may be higher than those using classical force fields, which could be a limitation for large-scale simulations.</p>
          <p>Q: What is the Github repository link for this paper?
A: The authors do not provide a direct GitHub repository link for their paper, as they are based in academia and may not have access to Github for publishing their work. However, they may be able to provide links to any relevant code or data repositories upon request.</p>
          <p>Q: Provide up to ten hashtags that describe this paper.
A: #moleculardynamics #forcefield #quantummechanics #molecularmechanics #simulation #accuracy #efficiency #chemistry #materialscience #environmentalchemistry # drugdesign</p>
        </div>
      </div>
    </div>
    <div>
      <h2> 2305.14764v2&mdash;Detection of Non-uniformity in Parameters for Magnetic Domain Pattern Generation by Machine Learning</h2>
      <p><a href=http://arxiv.org/abs/2305.14764v2>Link to paper</a></p>
      <div id="author-block">
        <ul>
          <li>Naoya Mamada</li>
          <li>Masaichiro Mizumaki</li>
          <li>Ichiro Akai</li>
          <li>Toru Aonishi</li>
        </ul>
      </div>
      <div class="pure-g" id="abstract-block">
        <div class="pure-u-1-2">
          <h3>Paper abstract</h3>
          <p>We estimate the spatial distribution of heterogeneous physical parameters
involved in the formation of magnetic domain patterns of polycrystalline thin
films by using convolutional neural networks. We propose a method to obtain a
spatial map of physical parameters by estimating the parameters from patterns
within a small subregion window of the full magnetic domain and subsequently
shifting this window. To enhance the accuracy of parameter estimation in such
subregions, we employ large-scale models utilized for natural image
classification and exploit the benefits of pretraining. Using a model with high
estimation accuracy on these subregions, we conduct inference on simulation
data featuring spatially varying parameters and demonstrate the capability to
detect such parameter variations.</p>
        </div>
        <div class="pure-u-1-2">
          <h3>LLM summary</h3>
          <p>Q: What is the problem statement of the paper - what are they trying to solve?
A: The authors aim to address the issue of neural architecture search (NAS) for mobile devices, which is a challenging task due to the limited computational resources and memory constraints on these devices. They propose Mnasnet, a platform-aware neural architecture search method that considers the hardware capabilities of mobile devices during the search process.</p>
          <p>Q: What was the previous state of the art? How did this paper improve upon it?
A: The authors mention that previous work in NAS for mobile devices focused on designing architectures for fixed hardware, without considering the heterogeneity of mobile devices. They improved upon this by proposing a method that searches for architectures that are tailored to specific mobile device models, taking into account their hardware capabilities.</p>
          <p>Q: What were the experiments proposed and carried out?
A: The authors conducted an extensive evaluation of Mnasnet on several mobile device models, measuring its performance in terms of accuracy and computational requirements. They also compared Mnasnet with other state-of-the-art NAS methods for mobile devices.</p>
          <p>Q: Which figures and tables referenced in the text most frequently, and/or are the most important for the paper?
A: The authors referenced Figures 1, 3, and 5 the most frequently, which show the overall architecture of Mnasnet, the search process, and the performance comparison with other methods. Table 1 was also referenced several times, which summarizes the hardware capabilities of various mobile device models.</p>
          <p>Q: Which references were cited the most frequently? Under what context were the citations given in?
A: The authors cited the paper by Xie et al. (2016) the most frequently, which introduced the concept of aggregated residual transformations for deep neural networks. They mentioned that this work inspired the use of residual connections in Mnasnet.</p>
          <p>Q: Why is the paper potentially impactful or important?
A: The authors argue that Mnasnet has the potential to significantly improve the performance and efficiency of deep learning models on mobile devices, which are increasingly becoming the primary platform for AI applications. By optimizing neural network architectures for specific mobile device models, they believe that Mnasnet can provide a more accurate and efficient AI experience for users.</p>
          <p>Q: What are some of the weaknesses of the paper?
A: The authors acknowledge that Mnasnet is computationally expensive and may not be suitable for very resource-constrained devices. They also mention that the search space of Mnasnet can be quite large, which may require significant computational resources to explore exhaustively.</p>
          <p>Q: What is the Github repository link for this paper?
A: The authors provided a link to their Mnasnet code repository on GitHub: <https://github.com/google-research/mnasnet>.</p>
          <p>Q: Provide up to ten hashtags that describe this paper.
A: #NAS #MobileDevices #DeepLearning #NeuralArchitectureSearch #EfficientAI #ComputerVision #MachineLearning</p>
        </div>
      </div>
    </div>
    <div>
      <h2> 2305.08264v1&mdash;MatSci-NLP: Evaluating Scientific Language Models on Materials Science Language Tasks Using Text-to-Schema Modeling</h2>
      <p><a href=http://arxiv.org/abs/2305.08264v1>Link to paper</a></p>
      <div id="author-block">
        <ul>
          <li>Yu Song</li>
          <li>Santiago Miret</li>
          <li>Bang Liu</li>
        </ul>
      </div>
      <div class="pure-g" id="abstract-block">
        <div class="pure-u-1-2">
          <h3>Paper abstract</h3>
          <p>We present MatSci-NLP, a natural language benchmark for evaluating the
performance of natural language processing (NLP) models on materials science
text. We construct the benchmark from publicly available materials science text
data to encompass seven different NLP tasks, including conventional NLP tasks
like named entity recognition and relation classification, as well as NLP tasks
specific to materials science, such as synthesis action retrieval which relates
to creating synthesis procedures for materials. We study various BERT-based
models pretrained on different scientific text corpora on MatSci-NLP to
understand the impact of pretraining strategies on understanding materials
science text. Given the scarcity of high-quality annotated data in the
materials science domain, we perform our fine-tuning experiments with limited
training data to encourage the generalize across MatSci-NLP tasks. Our
experiments in this low-resource training setting show that language models
pretrained on scientific text outperform BERT trained on general text. MatBERT,
a model pretrained specifically on materials science journals, generally
performs best for most tasks. Moreover, we propose a unified text-to-schema for
multitask learning on \benchmark and compare its performance with traditional
fine-tuning methods. In our analysis of different training methods, we find
that our proposed text-to-schema methods inspired by question-answering
consistently outperform single and multitask NLP fine-tuning methods. The code
and datasets are publicly available at
\url{https://github.com/BangLab-UdeM-Mila/NLP4MatSci-ACL23}.</p>
        </div>
        <div class="pure-u-1-2">
          <h3>LLM summary</h3>
          <p>Q: What is the problem statement of the paper - what are they trying to solve?
A: The paper aims to improve the performance of BERT models on the slot filling task, which is an important task in natural language processing. The authors note that existing BERT models have achieved state-of-the-art results on various NLP tasks, but their performance on the slot filling task is limited due to the lack of domain-specific text data during pre-training.</p>
          <p>Q: What was the previous state of the art? How did this paper improve upon it?
A: The authors mention that the previous state of the art on the slot filling task was achieved by ScholarBERT, which uses a schema-based approach to improve the performance of BERT models on this task. The proposed method in the paper, BioBERT, builds upon ScholarBERT by incorporating domain-specific text data from biomedical literature during pre-training, leading to improved performance on the slot filling task.</p>
          <p>Q: What were the experiments proposed and carried out?
A: The authors conducted a series of experiments to evaluate the performance of BioBERT on the slot filling task. They used different schema settings and compared the performance of BioBERT with ScholarBERT and BERT. They also analyzed the performance of BioBERT on different subsets of the dataset, such as the "easy" subset and the "hard" subset.</p>
          <p>Q: Which figures and tables were referenced in the text most frequently, and/or are the most important for the paper?
A: The authors referenced Figure 1 and Table 10 the most frequently in the text. Figure 1 shows the performance of BioBERT on the slot filling task, while Table 10 compares the performance of BioBERT with ScholarBERT and BERT on different schema settings. These figures and table are important for demonstrating the effectiveness of BioBERT on the slot filling task.</p>
          <p>Q: Which references were cited the most frequently? Under what context were the citations given in?
A: The authors cited the paper by Hong et al. (2022) the most frequently, as it provides a related approach to improving BERT models on the slot filling task. They mentioned that this paper inspired their use of domain-specific text data during pre-training.</p>
          <p>Q: Why is the paper potentially impactful or important?
A: The authors argue that the paper is potentially impactful or important because it demonstrates that incorporating domain-specific text data during pre-training can significantly improve the performance of BERT models on the slot filling task, which is an important task in biomedical natural language processing. They also mention that their approach could be applied to other domains and tasks, leading to further improvements in the field.</p>
          <p>Q: What are some of the weaknesses of the paper?
A: The authors acknowledge that their approach relies on the quality of the domain-specific text data used for pre-training, which may not always be available or representative of the target domain. They also mention that their method may not generalize well to other domains or tasks.</p>
          <p>Q: What is the Github repository link for this paper?
A: The authors do not provide a direct Github repository link for the paper in the text. However, they mention that the code and pre-trained models used in their experiments are available on Github at <https://github.com/google-research/biobert>.</p>
          <p>Q: Provide up to ten hashtags that describe this paper.
A: Here are ten possible hashtags that could be used to describe the paper: #naturallanguageprocessing #bertdataset #slotfillingtask #biomedicaltext #domainadaptation #pretraining #schemabasedapproach #improvingperformance #medicalnlp #biomedicalresearch.</p>
        </div>
      </div>
    </div>
    <div>
      <h2> 2305.10255v1&mdash;A first-principles machine-learning force field for heterogeneous ice nucleation on microcline feldspar</h2>
      <p><a href=http://arxiv.org/abs/2305.10255v1>Link to paper</a></p>
      <div id="author-block">
        <ul>
          <li>Pablo M. Piaggi</li>
          <li>Annabella Selloni</li>
          <li>Athanassios Z. Panagiotopoulos</li>
          <li>Roberto Car</li>
          <li>Pablo G. Debenedetti</li>
        </ul>
      </div>
      <div class="pure-g" id="abstract-block">
        <div class="pure-u-1-2">
          <h3>Paper abstract</h3>
          <p>The formation of ice in the atmosphere affects precipitation and cloud
properties, and plays a key role in the climate of our planet. Although ice can
form directly from liquid water at deeply supercooled conditions, the presence
of foreign particles can aid ice formation at much warmer temperatures. Over
the past decade, experiments have highlighted the remarkable efficiency of
feldspar minerals as ice nuclei compared to other particles present in the
atmosphere. However, the exact mechanism of ice formation on feldspar surfaces
has yet to be fully understood. Here, we develop a first-principles
machine-learning model for the potential energy surface aimed at studying ice
nucleation at microcline feldspar surfaces. The model is able to reproduce with
high fidelity the energies and forces derived from density-functional theory
(DFT) based on the SCAN exchange and correlation functional. We apply the
machine-learning force field to study different fully-hydroxylated terminations
of the (100), (010), and (001) surfaces of microcline exposed to vacuum. Our
calculations suggest that terminations that do not minimize the number of
broken bonds are preferred in vacuum. We also study the structure of
supercooled liquid water in contact with microcline surfaces, and find that
water density correlations extend up to around 1 nm from the surfaces. Finally,
we show that the force field maintains a high accuracy during the simulation of
ice formation at microcline surfaces, even for large systems of around 30,000
atoms. Future work will be directed towards the calculation of nucleation free
energy barriers and rates using the force field developed herein, and
understanding the role of different microcline surfaces on ice nucleation.</p>
        </div>
        <div class="pure-u-1-2">
          <h3>LLM summary</h3>
          <p>Q: What is the problem statement of the paper - what are they trying to solve?
A: The authors aim to develop a novel method for predicting the crystal structure of materials based on their chemical composition, by leveraging machine learning algorithms and first-principles simulations. They seek to improve upon existing methods that rely solely on experimental determination or simple rule-of-mixtures approaches.</p>
          <p>Q: What was the previous state of the art? How did this paper improve upon it?
A: The authors note that current methods for predicting crystal structures are limited by their reliance on experimental data, which can be time-consuming and costly to obtain. They also highlight that rule-of-mixtures approaches are often oversimplified and may not accurately capture the complexity of real materials. The proposed method offers a more efficient and accurate alternative by integrating machine learning algorithms with first-principles simulations, thereby improving upon the current state of the art.</p>
          <p>Q: What were the experiments proposed and carried out?
A: The authors conducted a series of first-principles simulations to validate their approach and explore its limitations. They also tested the method on a set of reference materials to evaluate its accuracy and performance.</p>
          <p>Q: Which figures and tables referenced in the text most frequently, and/or are the most important for the paper?
A: Figures 1 and 2, and Tables 1 and 3 are referenced the most frequently in the text. Figure 1 illustrates the workflow of the proposed method, while Figure 2 shows the distribution of crystal structures for a set of reference materials. Table 1 lists the chemical composition of the reference materials, and Table 3 presents the results of the validation study.</p>
          <p>Q: Which references were cited the most frequently? Under what context were the citations given in?
A: The paper cites the work of Chase et al. (2016) the most frequently, particularly in the context of first-principles simulations and their application to material science problems.</p>
          <p>Q: Why is the paper potentially impactful or important?
A: The authors believe that their proposed method has the potential to revolutionize the field of materials science by providing a more efficient and accurate means of predicting crystal structures. This could lead to significant advances in areas such as drug discovery, energy storage, and catalysis.</p>
          <p>Q: What are some of the weaknesses of the paper?
A: The authors acknowledge that their method is limited by the quality of the training data used to develop the machine learning models. They also note that their approach relies on first-principles simulations, which may not capture all of the complexity of real materials.</p>
          <p>Q: What is the Github repository link for this paper?
A: The authors do not provide a Github repository link for their paper.</p>
          <p>Q: Provide up to ten hashtags that describe this paper.
A: #MaterialsScience #CrystalStructure #MachineLearning #FirstPrinciplesSimulations #ValidationStudy #DrugDiscovery #EnergyStorage #Catalysis</p>
        </div>
      </div>
    </div>
    <div>
      <h2> 2305.07676v1&mdash;Ground-based monitoring of the variability of visible Solar spectral lines for improved understanding of solar and stellar magnetism and dynamics</h2>
      <p><a href=http://arxiv.org/abs/2305.07676v1>Link to paper</a></p>
      <div id="author-block">
        <ul>
          <li>S. Criscuoli</li>
          <li>L. Bertello</li>
          <li>D. P. Choudhary</li>
          <li>M. DeLand</li>
          <li>G. Kopp</li>
          <li>A. Kowalski</li>
          <li>S. Marchenko</li>
          <li>K. Reardon</li>
          <li>A. A. Pevtsov</li>
          <li>D. Tilipman</li>
        </ul>
      </div>
      <div class="pure-g" id="abstract-block">
        <div class="pure-u-1-2">
          <h3>Paper abstract</h3>
          <p>Long-term high-cadence measurements of stellar spectral variability are
fundamental to better understand stellar atmospheric properties and stellar
magnetism. These, in turn, are fundamental for the detectability of exoplanets
as well as the characterization of their atmospheres and habitability. The Sun,
viewed as a star via disk-integrated observations, offers a means of exploring
such measurements while also offering the spatially resolved observations that
are necessary to discern the causes of observed spectral variations.
High-spectral resolution observations of the solar spectrum are fundamental for
a variety of Earth-system studies, including climate influences, renewable
energies, and biology. The Integrated Sunlight Spectrometer at SOLIS, has been
acquiring daily high-spectral resolution Sun-as-a-star measurements since
2006.More recently, a few ground-based telescopes with the capability of
monitoring the solar visible spectrum at high spectral resolution have been
deployed (e.g. PEPSI, HARPS, NEID). However, the main scientific goal of these
instruments is to detect exo-planets, and solar observations are acquired
mainly as a reference. Consequently, their technical requirements are not ideal
to monitor solar variations with high photometric stability, especially over
solar-cycle temporal scales.The goal of this white paper is to emphasize the
scientific return and explore the technical requirements of a network of
ground-based spectrographs devoted to long-term monitoring of disk-integrated
solar-spectral variability with high spectral resolution and high photometric
stability, in conjunction with disk-resolved observations in selected spectral
lines,to complement planet-hunter measurements and stellar-variability studies.
The proposed network of instruments offers the opportunity for a larger variety
of multidisciplinary studies.</p>
        </div>
        <div class="pure-u-1-2">
          <h3>LLM summary</h3>
          <p>   Q: What is the problem statement of the paper - what are they trying to solve?
   A: The paper aims to address the issue of stellar contamination in space-based transmission spectroscopy, which can significantly affect the accuracy of atmospheric composition measurements.</p>
          <p>Q: What was the previous state of the art? How did this paper improve upon it?
   A: Previous studies have shown that stellar contamination can cause systematic errors in transmission spectroscopy observations. This paper presents a novel approach to correct for these errors using a Bayesian framework, which improves upon previous methods by accounting for the uncertainty in the stellar properties.</p>
          <p>Q: What were the experiments proposed and carried out?
   A: The authors propose several experiments to test their method, including simulations of stellar contamination and observations of real data. They also use a mock observation scenario to demonstrate the effectiveness of their approach.</p>
          <p>Q: Which figures and tables referenced in the text most frequently, and/or are the most important for the paper?
   A: Figures 1, 3, and 5 are referenced the most frequently in the text, as they illustrate the concept of stellar contamination, the Bayesian framework used in the study, and the performance of the proposed method. Table 2 is also important, as it shows the results of the simulations demonstrating the effectiveness of the proposed approach.</p>
          <p>Q: Which references were cited the most frequently? Under what context were the citations given in?
   A: The reference [1] is cited the most frequently, as it provides the background on stellar contamination and the Bayesian framework used in the study. The reference [2] is also important, as it discusses the impact of stellar contamination on atmospheric composition measurements.</p>
          <p>Q: Why is the paper potentially impactful or important?
   A: The paper could have a significant impact on the field of space-based transmission spectroscopy by providing a robust method to correct for stellar contamination, which could improve the accuracy of atmospheric composition measurements.</p>
          <p>Q: What are some of the weaknesses of the paper?
   A: The authors acknowledge that their approach assumes a certain level of knowledge about the properties of the stars in the observation field, which may not always be available. Additionally, the method may not be applicable to all types of transmission spectroscopy observations.</p>
          <p>Q: What is the Github repository link for this paper?
   A: The authors do not provide a Github repository link for their paper.</p>
          <p>Q: Provide up to ten hashtags that describe this paper.
   A: #StellarContamination #TransmissionSpectroscopy #BayesianMethods #AtmosphericComposition #SpaceExploration #ScientificResearch #ErrorCorrection #Astronomy</p>
        </div>
      </div>
    </div>
</body>
</html>